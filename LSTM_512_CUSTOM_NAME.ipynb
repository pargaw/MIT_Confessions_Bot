{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, TimeDistributed,Masking \n",
    "from time import sleep\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'lstm_512_simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentences:\n",
      "- #979 I'm too scared to make eye contact with anyone because I'm scared they will find out how sad I am.`\n",
      "- #9264 i wear chokers but i don't do anal. no butt stuff whatsoever. like do NOT touch my asshole`\n",
      "\n",
      "{' ': 2, '$': 6, '(': 10, ',': 14, '0': 18, '4': 22, '8': 26, '<': 30, '@': 34, 'D': 38, 'H': 42, 'L': 46, 'P': 50, 'T': 54, 'X': 58, '`': 1, 'd': 68, 'h': 72, 'l': 76, 'p': 80, 't': 84, 'x': 88, '|': 91, '#': 5, \"'\": 9, '+': 13, '/': 17, '3': 21, '7': 25, ';': 29, '?': 33, 'C': 37, 'G': 41, 'K': 45, 'O': 49, 'S': 53, 'W': 57, '[': 61, '_': 64, 'c': 67, 'g': 71, 'k': 75, 'o': 79, 's': 83, 'w': 87, '{': 0, '\"': 4, '&': 8, '*': 12, '.': 16, '2': 20, '6': 24, ':': 28, '>': 32, 'B': 36, 'F': 40, 'J': 44, 'N': 48, 'R': 52, 'V': 56, 'Z': 60, '^': 63, 'b': 66, 'f': 70, 'j': 74, 'n': 78, 'r': 82, 'v': 86, 'z': 90, '~': 93, '!': 3, '%': 7, ')': 11, '-': 15, '1': 19, '5': 23, '9': 27, '=': 31, 'A': 35, 'E': 39, 'I': 43, 'M': 47, 'Q': 51, 'U': 55, 'Y': 59, ']': 62, 'a': 65, 'e': 69, 'i': 73, 'm': 77, 'q': 81, 'u': 85, 'y': 89, '}': 92}\n",
      "('# training samples:', 981562)\n"
     ]
    }
   ],
   "source": [
    "# change to False if you want to train on datasets \n",
    "# where we don't use CUSTOM_NAME token for tagged Facebook names\n",
    "custom_names = True\n",
    "\n",
    "if custom_names:\n",
    "    comments_filepath = \"csv_data/CUSTOM_NAMES/custom_name_token_comments.csv\"\n",
    "else:\n",
    "    comments_filepath = \"csv_data/FILTERED_NAMES/filtered_names_comments.csv\"\n",
    "\n",
    "status_filepath = \"csv_data/beaverconfessions_facebook_statuses.csv\"\n",
    "        \n",
    "stop_symbol = \"`\"\n",
    "\n",
    "def load_dataset(filepath, dictionary, datatype):\n",
    "    dt = \"{}_message\".format(datatype)\n",
    "    \n",
    "    with open(filepath, \"rU\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        sentences = []\n",
    "        \n",
    "        for status in reader:\n",
    "            if dt not in status:\n",
    "                # 2 rows are being read at a time for the statuses csv for some reason...\n",
    "                # handled with monkey patching below\n",
    "                msg1, msg2 = status.items()[1]\n",
    "\n",
    "                for char in msg1:\n",
    "                    dictionary.add(char)\n",
    "                for char in msg2:\n",
    "                    dictionary.add(char)\n",
    "                \n",
    "                # add stop symbol to end before text->int conversion\n",
    "                msg1 += stop_symbol\n",
    "                msg2 += stop_symbol\n",
    "                sentences.append(msg1)\n",
    "                sentences.append(msg2)\n",
    "            else:\n",
    "                msg = status[dt]\n",
    "                for char in msg:\n",
    "                    dictionary.add(char)\n",
    "                msg += stop_symbol\n",
    "                sentences.append(msg)\n",
    "                    \n",
    "    return dictionary, sentences\n",
    "    \n",
    "dictionary, comments = load_dataset(comments_filepath, set([]), \"comment\")\n",
    "dictionary, statuses = load_dataset(status_filepath, dictionary, \"status\")\n",
    "sentences = []\n",
    "sentences.extend(comments)\n",
    "sentences.extend(statuses)\n",
    "sentences_arr = np.array(sentences)\n",
    "\n",
    "# shuffle statuses and comments\n",
    "random_permutation = np.random.permutation(len(sentences))\n",
    "sentences_arr = sentences_arr[random_permutation]\n",
    "\n",
    "print 'Sample sentences:'\n",
    "print '-', sentences_arr[0]\n",
    "print '-', sentences_arr[1]\n",
    "print\n",
    "\n",
    "# create vocabulary of characters found in data\n",
    "chars = sorted(list(dictionary))\n",
    "padding_symbol = \"{\"\n",
    "chars.insert(0, stop_symbol) # index 1\n",
    "chars.insert(0, padding_symbol) # index 0 \n",
    "# print chars\n",
    "\n",
    "# print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "nb_classes = len(char_indices)\n",
    "print char_indices\n",
    "\n",
    "\n",
    "# TODO separate model_related values and preprocessing code\n",
    "\n",
    "def convert_sentences_to_subsentences(sentences, max_len, window_size=10, step_size=1):\n",
    "    sub_sentences = []\n",
    "    next_sub_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(0, max_len - window_size + 1, step_size):\n",
    "            sub_sentences.append(sentence[i : i+window_size])\n",
    "            next_sub_sentences.append(sentence[i+1 : i+1+window_size])\n",
    "    \n",
    "    return sub_sentences, next_sub_sentences\n",
    "\n",
    "maxlen = 280\n",
    "sub_sentences, next_sub_sentences = convert_sentences_to_subsentences(sentences, maxlen)\n",
    "nb_samples = len(sub_sentences)\n",
    "\n",
    "# any values not filled in represent padding \n",
    "# extra 1 represents space for the stop symbol\n",
    "X_labels = np.zeros((nb_samples, maxlen+1))\n",
    "y_labels = np.zeros((nb_samples, maxlen+1))\n",
    "\n",
    "for sample_nb in range(nb_samples):\n",
    "    x_label = map(lambda x: char_indices[x], sub_sentences[sample_nb])\n",
    "    y_label = map(lambda x: char_indices[x], next_sub_sentences[sample_nb])\n",
    "\n",
    "    X_labels[sample_nb][:len(x_label)] = x_label\n",
    "    y_labels[sample_nb][:len(y_label)] = y_label\n",
    "\n",
    "# print X_labels[:1], y_labels[:1]\n",
    "\n",
    "print('# training samples:', nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# make use of subsentences, next_chars, which are our words (semantic, not encoded yet)\n",
    "\n",
    "def validate_data(X, y):\n",
    "    # assert at least one nonzero value in each OHE sample\n",
    "    assert(False not in np.any(X>0, axis=1))\n",
    "    assert(False not in np.any(y>0, axis=1))\n",
    "\n",
    "def sampling_generator(epoch_size, batch_size, validation=False, sample_weights=False):  \n",
    "    \"\"\"\n",
    "    Takes in labels of int values, e.g. [1. 2. 5. 9. 1. 0. 0. ...]\n",
    "    These labels are already padded with 0s to a predetermined maxlen.\n",
    "    We convert each sample X and y to one hot versions, and create batches.\n",
    "    Batch generation is necessary to avoid MemoryError.\n",
    "    \"\"\"\n",
    "    \n",
    "    if validation:\n",
    "        X_labels = X_val_labels \n",
    "        y_labels = y_val_labels \n",
    "    else:\n",
    "        X_labels = X_train_labels\n",
    "        y_labels = y_train_labels\n",
    "    \n",
    "    while True:\n",
    "        # hand off data in batches\n",
    "        for i in range(int(epoch_size/batch_size)):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, epoch_size)\n",
    "            true_batch_size = end - start\n",
    "            \n",
    "            # fresh batch\n",
    "            sample_X = []\n",
    "            sample_Y = []\n",
    "\n",
    "            # one hot encode\n",
    "            for i in range(true_batch_size):                \n",
    "                ohe_x = to_categorical(X_labels[i], num_classes=nb_classes)\n",
    "                ohe_y = to_categorical(y_labels[i], num_classes=nb_classes)\n",
    "                \n",
    "                # ensure no 0 zeros in data - otherwise training NaNs out\n",
    "                # validate_data(ohe_x, ohe_y)\n",
    "                \n",
    "                # add dim of size 1 to front of np arrays to allow for vstacking\n",
    "                ohe_x = np.expand_dims(ohe_x, axis=0)\n",
    "                ohe_y = np.expand_dims(ohe_y, axis=0)\n",
    "                                \n",
    "                # build the batch\n",
    "                if len(sample_X) == 0:\n",
    "                    sample_X = ohe_x\n",
    "                    sample_Y = ohe_y\n",
    "                else:\n",
    "                    sample_X = np.vstack((sample_X, ohe_x))\n",
    "                    sample_Y = np.vstack((sample_Y, ohe_y))\n",
    "                        \n",
    "            if sample_weights:\n",
    "                # TODO fill in with reactions if so desired :)\n",
    "                sample_weights = []\n",
    "                yield (sample_x, sample_y, sample_weights)\n",
    "            \n",
    "            yield (sample_X, sample_Y)\n",
    "\n",
    "# print next(sampling_generator())          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model is made!\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0,input_shape=(maxlen+1, nb_classes)))\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen+1, nb_classes)))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(nb_classes)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam') \n",
    "print ('Model is made!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'twopi': '/home/ubuntu/anaconda2/bin/twopi', 'fdp': '/home/ubuntu/anaconda2/bin/fdp', 'circo': '/home/ubuntu/anaconda2/bin/circo', 'neato': '/home/ubuntu/anaconda2/bin/neato', 'dot': '/home/ubuntu/anaconda2/bin/dot', 'sfdp': '/home/ubuntu/anaconda2/bin/sfdp'}\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "print pydot.find_graphviz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-fbde26a53eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{}_viz.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/vis_utils.pyc\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/vis_utils.pyc\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/vis_utils.pyc\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# pydot raises a generic Exception here,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# so no specific class can be caught.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[1;32m     28\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='{}_viz.png'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 281, 94)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 281, 512)          1243136   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 281, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 281, 512)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 281, 94)           48222     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 281, 94)           0         \n",
      "=================================================================\n",
      "Total params: 3,390,558\n",
      "Trainable params: 3,390,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# split into train and val data\n",
    "X_train_labels, X_val_labels, y_train_labels, y_val_labels = train_test_split(X_labels, y_labels, test_size=0.3)\n",
    "nb_train_samples = len(X_train_labels) #300\n",
    "nb_val_samples = len(X_val_labels) #300\n",
    "batch_size = 128 #100\n",
    "\n",
    "class PlotHistory(Callback):\n",
    "    def __init__(self, run_name):\n",
    "        self.run_name = run_name \n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # losses\n",
    "        loss_handles = []\n",
    "        for key in self.history:\n",
    "            l, = plt.plot(self.history[key], label=key)\n",
    "            loss_handles.append(l)\n",
    "\n",
    "        plt.title('losses and metrics for {}'.format(self.run_name))    \n",
    "        plt.ylabel('loss')\n",
    "        plt.yscale('symlog')\n",
    "        plt.legend(handles=loss_handles, fontsize=6)          \n",
    "\n",
    "        # make subplots close to each other and hide x ticks for all but bottom plot\n",
    "        plt.savefig('{}_plot.jpg'.format(self.run_name))        \n",
    "        plt.clf()\n",
    "        print \"Epoch:\",epoch\n",
    "\n",
    "# callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='{}.hdf5'.format(model_name), verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('{}.log'.format(model_name))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "plot_history = PlotHistory(model_name)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                                              verbose=1, epsilon=1e-4, min_lr=1E-6) \n",
    "\n",
    "# train\n",
    "history = model.fit_generator(\n",
    "    sampling_generator(nb_train_samples, batch_size), \n",
    "    steps_per_epoch=nb_train_samples/batch_size,\n",
    "    epochs=10, #2,\n",
    "    verbose=1,\n",
    "    validation_data=sampling_generator(nb_val_samples, batch_size, validation=True),\n",
    "    validation_steps=nb_val_samples/batch_size,\n",
    "    callbacks=[early_stopping, reduce_lr, csv_logger, checkpointer, plot_history])\n",
    "    \n",
    "sleep(0.1) # https://github.com/fchollet/keras/issues/2110 \n",
    "\n",
    "hist = history.history\n",
    "print 'Loss and val_loss is', hist['loss'][0], hist['val_loss'][0]\n",
    "\n",
    "print history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('{}.hdf5'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5867 havend\n",
      "#5867 havend{\n",
      "#5867 havend{{\n",
      "#5867 havend{{{\n",
      "#5867 havend{{{{\n",
      "#5867 havend{{{{{\n",
      "#5867 havend{{{{{{\n",
      "#5867 havend{{{{{{{\n",
      "#5867 havend{{{{{{{{\n",
      "#5867 havend{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n",
      "#5867 havend{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-3b61c2937ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#5867 haven\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mgenerate_confession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-3b61c2937ed2>\u001b[0m in \u001b[0;36mgenerate_confession\u001b[0;34m(model, seed_string)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# get next char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# otherwise wrapped in (1,maxlen+1,len(chars))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1748\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def convert_sentence_to_ohe(sentence):\n",
    "    x_label = map(lambda x: char_indices[x], sentence)\n",
    "    confession = np.zeros(maxlen+1)\n",
    "    confession[:len(x_label)] = x_label\n",
    "    ohe_x = to_categorical(confession, num_classes=nb_classes)\n",
    "    return np.expand_dims(ohe_x, axis=0)\n",
    "\n",
    "def generate_confession(model, seed_string):\n",
    "    # nb chars to preserve\n",
    "    orig_len = len(seed_string) \n",
    "    confession = seed_string\n",
    "    finalStr = seed_string\n",
    "    \n",
    "    for char_nb in range(orig_len, maxlen):\n",
    "        x = convert_sentence_to_ohe(confession)\n",
    "        \n",
    "        # get next char\n",
    "        preds = model.predict(x)[0] # otherwise wrapped in (1,maxlen+1,len(chars))\n",
    "        best_tokens = np.argmax(preds, axis=1)\n",
    "        \n",
    "        # char_nb-1 because we want prev val in y matrix (best_tokens). In training we treat y as a shifted \n",
    "            # version of x hence offset -1 here\n",
    "        next_token = best_tokens[char_nb-1] \n",
    "#         print next_token\n",
    "        # stop symbol\n",
    "        if next_token == 1:\n",
    "            break\n",
    "            \n",
    "        next_char = indices_char[next_token]\n",
    "        \n",
    "        finalStr += next_char\n",
    "#         moveConfess = confession[1:]\n",
    "        confession += next_char\n",
    "#         print confession\n",
    "        \n",
    "        print finalStr\n",
    "                \n",
    "    return confession\n",
    "\n",
    "seed_string=\"#5867 haven\"\n",
    "print generate_confession(model, seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
