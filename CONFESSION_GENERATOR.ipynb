{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBALS AND INITIAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot import emo_unicode\n",
    "from keras.callbacks import Callback, EarlyStopping, CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Embedding, Conv1D, Dropout, LSTM, TimeDistributed, GRU, Masking \n",
    "from keras.models import load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize #, wordpunct_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import sleep\n",
    "import csv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "# CHANGE PATH TO MODEL OUTPUTS HERE BASED ON YOUR LOCAL CONFIG\n",
    "BASE_PATH = '/Users/goldenzenith/Dropbox (MIT)/6.867_saved_models/'\n",
    "FOLDER = 'GRU_WORDWORD/'\n",
    "PATH = BASE_PATH + FOLDER\n",
    "print 'Model files will be saved to:', PATH\n",
    "\n",
    "# plots, logs, weight files will be based on this\n",
    "# plz name descriptively, e.g. lstm_512_charchar\n",
    "MODEL_NAME = 'gru_512_wordword' \n",
    "\n",
    "# approach\n",
    "CHAR_BY_CHAR = False # True for character-by-character training\n",
    "CUSTOM_NAMES = True # True to use CUSTOM_NAME token (not '') for tagged FB names\n",
    "\n",
    "if CUSTOM_NAMES:\n",
    "    COMMENTS_FILEPATH = \"csv_data/CUSTOM_NAMES/custom_name_token_comments.csv\"\n",
    "else:\n",
    "    COMMENTS_FILEPATH = \"csv_data/FILTERED_NAMES/filtered_names_comments.csv\"\n",
    "\n",
    "# paths with data based on filtering done in adapted version of Facebook post scraping\n",
    "STATUS_FILEPATH = \"csv_data/beaverconfessions_facebook_statuses.csv\" \n",
    "\n",
    "# parameters/settings\n",
    "# MAX_LEN is defined later\n",
    "EMBEDDING = False # True to use CNN\n",
    "if EMBEDDING:\n",
    "    CONV_NB_FILTERS = 100\n",
    "    CONV_KERNEL_SIZE = 3\n",
    "LSTM_MODEL = False # False to use GRU\n",
    "HIDDEN_UNITS = 512\n",
    "STEP_SIZE = 1\n",
    "WINDOW_SIZE = 10 \n",
    "\n",
    "# sample weights\n",
    "WEIGHT_BY_REACTIONS = False # False to give all samples same weight\n",
    "LIKES_ONLY = False # True for only likes, not all reactions\n",
    "\n",
    "# special symbols\n",
    "CUSTOM_NAME = '<name>'\n",
    "CUSTOM_NUMBER = '<number>'\n",
    "PADDING_SYMBOL = '<padding>'\n",
    "STOP_SYMBOL = '<stahp>'\n",
    "CUSTOM_SYMBOLS = [CUSTOM_NAME, CUSTOM_NUMBER, PADDING_SYMBOL, STOP_SYMBOL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING (TOKENIZATION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "escaped_punctuation = re.escape(string.punctuation)\n",
    "# print 'Escaped punctuation:', escaped_punctuation\n",
    "\n",
    "# insert OR pipe before each punctuation mark\n",
    "xor_punctuation = '|'.join('{}{}+'.format(escaped_punctuation[x], escaped_punctuation[x+1]) for x in range(0, len(escaped_punctuation), 2))\n",
    "# print 'Delimited punctuation:', xor_punctuation\n",
    "\n",
    "# build regex with variable (order matters!)\n",
    "nm = CUSTOM_NAME.lower()\n",
    "nb = CUSTOM_NUMBER.lower()\n",
    "pd = PADDING_SYMBOL.lower()\n",
    "sp = STOP_SYMBOL.lower()\n",
    "course_number = '\\d+\\.\\d{2,3}'\n",
    "multiple_numbers = '\\d+'\n",
    "emoticon_pattern = '|'.join(emoticon for emoticon in emo_unicode.EMOTICONS)\n",
    "space = '\\s'\n",
    "regex_expr = r'(' + '|'.join([nm, nb, pd, sp, course_number, multiple_numbers, emoticon_pattern, xor_punctuation, space]) + r')'\n",
    "# print '\\nRegex expression:', regex_expr\n",
    "\n",
    "def replace_digit_with_token(string): \n",
    "    return CUSTOM_NUMBER.lower() if string.isdigit() else string\n",
    "\n",
    "def tokenize_str(string, replace_digits=True):\n",
    "    \"\"\"\n",
    "    NLTK tokenizers (word_tokenize, wordpunct_tokenize) are insufficient, \n",
    "    as emoticons and course #s, e.g. 6.111, are important to our dataset.\n",
    "    \n",
    "    This is a custom tokenizer to retain such items in the vocab,\n",
    "        but split up other words containing numbers/punctuation, e.g. 3pm.\n",
    "        \n",
    "    Note that these texts are lowercased by default prior to tokenization.\n",
    "    \"\"\"\n",
    "    if CHAR_BY_CHAR:\n",
    "        tokens = list(string)\n",
    "        \n",
    "    else:\n",
    "        tokens = re.split(regex_expr, string.lower())\n",
    "\n",
    "        # filter out spaces\n",
    "        tokens = [token for token in tokens if token not in [\"\", \" \"]]\n",
    "\n",
    "        # tokens with *just* digits are mapped to <NUMBER> by default\n",
    "        if replace_digits:\n",
    "            tokens = map(lambda x: replace_digit_with_token(x), tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# attempt to catch major cases in our dataset\n",
    "sentence = CUSTOM_NAME + '6.867test. @7:30pm in 54-100. The profs, they\\'re coming in 3..2..1. LETS DOOO THIS!!! >;)' + STOP_SYMBOL + PADDING_SYMBOL\n",
    "print 'Test case:', tokenize_str(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_vocabulary(filepath, vocabulary, datatype, max_nb_tokens, reactions):\n",
    "    \"\"\"\n",
    "    Read individual sentences into memory, and generate vocabulary.\n",
    "    \n",
    "    If CHAR_BY_CHAR, the vocabulary will hold single characters, \n",
    "        e.g. a-zA-Z and punctuation.\n",
    "    Else, it will contain whole words and Unicode 'phrases',\n",
    "        e.g. :\\'(, as split by our custom tokenizer.\n",
    "    \"\"\"\n",
    "    dt = \"{}_message\".format(datatype)\n",
    "    reaction_header = 'num_likes' if LIKES_ONLY else 'num_reactions'    \n",
    "    \n",
    "    with open(filepath, \"rU\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        tokens = []\n",
    "        \n",
    "        for status in reader:\n",
    "            # we distinguish between comments and statuses with header cols in the CSVs\n",
    "            \n",
    "            if dt not in status:\n",
    "                # 2 rows are read at once from STATUS_FILEPATH for some reason...\n",
    "                # handled with monkey patching \n",
    "                msg1, msg2 = status.items()[1]\n",
    "                \n",
    "                # append stop symbol so model has explicit marker for ending\n",
    "                msg1 += STOP_SYMBOL\n",
    "                msg2 += STOP_SYMBOL\n",
    "                \n",
    "                # we define elts as the unit we are training on, e.g. char vs. word \n",
    "                elts_in_sentence1 = tokenize_str(msg1)\n",
    "                elts_in_sentence2 = tokenize_str(msg2)\n",
    "                \n",
    "                max_nb_tokens = max(max_nb_tokens, max(len(elts_in_sentence1), len(elts_in_sentence2)))\n",
    "                 \n",
    "                for elt in elts_in_sentence1:\n",
    "                    vocabulary.add(elt)\n",
    "                for elt in elts_in_sentence2:\n",
    "                    vocabulary.add(elt) \n",
    "                    \n",
    "                # build list of tokens in local memory\n",
    "                tokens.append(elts_in_sentence1)\n",
    "                tokens.append(elts_in_sentence2)\n",
    "            else:\n",
    "                msg = status[dt]\n",
    "                nb_reactions = float(status[reaction_header])\n",
    "                \n",
    "                msg += STOP_SYMBOL \n",
    "                elts_in_sentence = tokenize_str(msg)\n",
    "                max_nb_tokens = max(max_nb_tokens, len(elts_in_sentence))\n",
    "                    \n",
    "                for elt in elts_in_sentence:\n",
    "                    vocabulary.add(elt)\n",
    "                    \n",
    "                tokens.append(elts_in_sentence)\n",
    "                reactions.append(nb_reactions)\n",
    "                    \n",
    "    return vocabulary, tokens, max_nb_tokens, reactions\n",
    "    \n",
    "# load comments and statuses separately at first\n",
    "max_nb_tokens = 0\n",
    "\n",
    "vocabulary, comment_tokens, max_nb_tokens, comment_reactions = \\\n",
    "    load_dataset_and_vocabulary(COMMENTS_FILEPATH, set([]), \"comment\", max_nb_tokens, [])\n",
    "\n",
    "vocabulary, status_tokens, max_nb_tokens, reactions = \\\n",
    "    load_dataset_and_vocabulary(STATUS_FILEPATH, vocabulary, \"status\", max_nb_tokens, comment_reactions)\n",
    "\n",
    "MAX_LEN = max_nb_tokens # includes stop symbol\n",
    "\n",
    "max_nb_reactions = max(reactions)\n",
    "print '\\nMax # of reactions:', max_nb_reactions\n",
    "\n",
    "print 'Max # of tokens in a sentence:', MAX_LEN\n",
    "print '\\nSample of 10 processed sentences:'\n",
    "for status in status_tokens[:5]:\n",
    "    print status\n",
    "    \n",
    "# create vocabulary of characters found in data\n",
    "if not CHAR_BY_CHAR:\n",
    "    vocabulary.add(CUSTOM_NAME)\n",
    "    vocabulary.add(CUSTOM_NUMBER)\n",
    "    \n",
    "# these symbols did not exist in the original training set,\n",
    "# so we can safely add them \n",
    "# NOTE: changed from insert(0,_), since we were adding duplicates\n",
    "vocabulary.add(STOP_SYMBOL)\n",
    "vocabulary.add(PADDING_SYMBOL)\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "\n",
    "NB_CLASSES = len(vocabulary)\n",
    "\n",
    "tokens_indices = dict((t, i) for i, t in enumerate(vocabulary))\n",
    "indices_tokens = {v: k for k, v in tokens_indices.iteritems()}\n",
    "PADDING_INDEX = tokens_indices[PADDING_SYMBOL]\n",
    "STOP_INDEX = tokens_indices[STOP_SYMBOL]\n",
    "\n",
    "print '\\nPadding index:', PADDING_INDEX, 'and stop index:', STOP_INDEX\n",
    "\n",
    "input_type = \"character by character\" if CHAR_BY_CHAR else \"word by word\"\n",
    "# print \"For\", input_type, \", we have this vocabulary:\", vocabulary, \"\\n of size\", NB_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD TRAINING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge comments and statuses\n",
    "tokens = []\n",
    "tokens.extend(comment_tokens)\n",
    "tokens.extend(status_tokens)\n",
    "tokens = np.array(tokens)\n",
    "\n",
    "print 'Sample tokenized sentence:'\n",
    "print tokens[0]\n",
    "print\n",
    "\n",
    "def generate_X_and_Y(sentences):\n",
    "    \"\"\"\n",
    "    X = sub_sentences, Y = next_sub_sentences\n",
    "    Y is simply X shifted over by step_size.\n",
    "    We want sub sentences per sentence to create multiple samples.\n",
    "    \"\"\"\n",
    "    sub_sentences = []\n",
    "    next_sub_sentences = []\n",
    "    \n",
    "    sub_reactions = []\n",
    "    \n",
    "    for sentence_nb, sentence in enumerate(sentences):\n",
    "        nb_sentence_reactions = reactions[sentence_nb]\n",
    "        \n",
    "        for i in range(0, len(sentence) - WINDOW_SIZE, STEP_SIZE):\n",
    "            sub_reactions.append(nb_sentence_reactions)\n",
    "            sub_sentences.append(sentence[i : i+WINDOW_SIZE])\n",
    "            next_sub_sentences.append(sentence[(i+STEP_SIZE) : (i+STEP_SIZE)+WINDOW_SIZE])\n",
    "    \n",
    "    return sub_sentences, next_sub_sentences, sub_reactions\n",
    "\n",
    "X, Y, Reacts = generate_X_and_Y(tokens)\n",
    "nb_samples = len(X)\n",
    "print 'Sample X:', X[0]\n",
    "print 'Sample Y:', Y[0]\n",
    "print \n",
    "\n",
    "def convert_tokens_to_int(X, Y, nb_samples):\n",
    "    \"\"\"\n",
    "    Convert token to integer representations.\n",
    "    \n",
    "    Populate label matrices with ints,\n",
    "    so result is padded and ready for training.\n",
    "    \"\"\"\n",
    "    # IGNORE THIS COMMENT but DON'T REMOVE -> extra 1 represents space for the stop symbol\n",
    "    input_shape = (nb_samples, MAX_LEN)\n",
    "    X_labels = np.zeros(input_shape)\n",
    "    y_labels = np.zeros(input_shape)\n",
    "\n",
    "    for sample_nb in range(nb_samples):\n",
    "        x_label = map(lambda x: tokens_indices[x], X[sample_nb]) \n",
    "        y_label = map(lambda x: tokens_indices[x], Y[sample_nb])\n",
    "\n",
    "        X_labels[sample_nb][:len(x_label)] = x_label\n",
    "        X_labels[sample_nb][len(x_label):] = PADDING_INDEX\n",
    "        y_labels[sample_nb][:len(y_label)] = y_label\n",
    "        y_labels[sample_nb][len(y_label):] = PADDING_INDEX\n",
    "    \n",
    "    return X_labels, y_labels\n",
    "\n",
    "X_labels, y_labels = convert_tokens_to_int(X, Y, nb_samples)\n",
    "\n",
    "print 'Sample X labels:\\n', X_labels[:1]\n",
    "print 'Sample Y labels:\\n', y_labels[:1]\n",
    "print '\\nTOTAL NB OF TRAINING SAMPLES:', nb_samples\n",
    "\n",
    "# split data into train and val sets\n",
    "X_train_labels, X_val_labels, y_train_labels, y_val_labels = \\\n",
    "    train_test_split(X_labels, y_labels, test_size=0.3)\n",
    "X_train_reacts, X_val_reacts = \\\n",
    "    Reacts[:len(X_train_labels)], Reacts[len(X_train_labels):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(X, y):\n",
    "    # assert at least 1 nonzero value in each OHE sample\n",
    "    assert(False not in np.any(X>0, axis=1))\n",
    "    assert(False not in np.any(y>0, axis=1))\n",
    "\n",
    "def sampling_generator(epoch_size, batch_size, validation=False):  \n",
    "    \"\"\"\n",
    "    Takes in labels of int values, e.g. [1. 2. 5. 9. 1. 0. 0. ...]\n",
    "    These labels are already padded with 0s to a predetermined maxlen.\n",
    "    We convert each sample X and y to one hot versions, and create batches.\n",
    "    Batch generation is necessary to avoid MemoryError.\n",
    "    \"\"\"\n",
    "    if validation:\n",
    "        X_labels = X_val_labels\n",
    "        y_labels = y_val_labels \n",
    "        react_labels = X_val_reacts\n",
    "    else:\n",
    "        X_labels = X_train_labels\n",
    "        y_labels = y_train_labels\n",
    "        react_labels = X_train_reacts\n",
    "    \n",
    "    while True:\n",
    "        # hand off data in batches\n",
    "        for i in range(int(epoch_size/batch_size)):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, epoch_size)\n",
    "            true_batch_size = end - start\n",
    "\n",
    "            # fresh batch\n",
    "            batch_X = []\n",
    "            batch_Y = []\n",
    "            batch_shape = (true_batch_size, MAX_LEN, NB_CLASSES)\n",
    "            \n",
    "            X_label_sample = X_labels[start:end]\n",
    "            X_label_sample_reacts = react_labels[start:end]\n",
    "\n",
    "            # one hot encode\n",
    "            if EMBEDDING:\n",
    "                batch_X = X_label_sample\n",
    "            else:\n",
    "                batch_X = to_categorical(X_label_sample, num_classes=NB_CLASSES).reshape(batch_shape)\n",
    "            \n",
    "            batch_Y = to_categorical(y_labels[start:end], num_classes=NB_CLASSES).reshape(batch_shape)                    \n",
    "            \n",
    "            total_nb_one_hots = X_label_sample.size\n",
    "            sample_weights = np.ones((total_nb_one_hots, 1))\n",
    "            \n",
    "            temp_reshaped_Xs = np.ravel(X_label_sample).reshape(-1)\n",
    "            padded_positions = np.where(temp_reshaped_Xs == PADDING_INDEX)\n",
    "            sample_weights[padded_positions] = 0 # 28100 x 1\n",
    "            sample_weights = sample_weights.reshape(true_batch_size, MAX_LEN)\n",
    "            \n",
    "            if WEIGHT_BY_REACTIONS:\n",
    "                # turn into col vector\n",
    "                react_weights = np.array(X_label_sample_reacts).reshape(-1,1)\n",
    "                react_weights = (react_weights / max_nb_reactions) + 1\n",
    "                \n",
    "                sample_weights = np.multiply(sample_weights, react_weights)\n",
    "            \n",
    "            yield (batch_X, batch_Y, sample_weights)\n",
    "            \n",
    "# next(sampling_generator(200, 128, validation=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "    \n",
    "    if EMBEDDING:\n",
    "        model.add(Embedding(input_dim=NB_CLASSES, output_dim=64, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(CONV_NB_FILTERS, CONV_KERNEL_SIZE, padding='same', activation='linear', strides=1))\n",
    "        model.add(Activation('relu'))\n",
    "        next_inp_shape = (MAX_LEN, CONV_NB_FILTERS)\n",
    "    else:\n",
    "        next_inp_shape = (MAX_LEN, NB_CLASSES)        \n",
    "    \n",
    "    if LSTM_MODEL:\n",
    "        model.add(LSTM(512, return_sequences=True, input_shape=next_inp_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(512, return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(NB_CLASSES)))\n",
    "    else:\n",
    "        model.add(GRU(512, return_sequences=True, input_shape=next_inp_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(512, return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(NB_CLASSES)))\n",
    "\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # to apply sample weights to metrics, specify weighted_metrics=[list of metrics]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.0), sample_weight_mode=\"temporal\") \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALLBACKS (bells + whistles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotHistory(Callback):\n",
    "    def __init__(self, path, run_name):\n",
    "        self.path = path\n",
    "        self.run_name = run_name \n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # create loss and perplexity plot\n",
    "        loss_handles = []\n",
    "        train_perp = []\n",
    "        val_perp = []\n",
    "        for key in self.history:           \n",
    "            if key != \"lr\":\n",
    "                l, = plt.plot(self.history[key], label=key)\n",
    "                loss_handles.append(l)\n",
    "\n",
    "        plt.title('Losses and metrics for {}'.format(self.run_name))    \n",
    "        plt.ylabel('loss')\n",
    "        plt.yscale('symlog')\n",
    "        plt.legend([\"Train Loss\",\"Val Loss\"], fontsize=8, loc='upper right')          \n",
    "        plt.savefig('{}_plot.jpg'.format(self.path+self.run_name))        \n",
    "        plt.clf()\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='{}.hdf5'.format(PATH+MODEL_NAME), verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('{}.log'.format(PATH+MODEL_NAME))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, verbose=1)\n",
    "plot_history = PlotHistory(PATH, MODEL_NAME)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, epsilon=1e-4, min_lr=1E-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerplexity(path, name):\n",
    "    f = open(path+name+'.log','r')\n",
    "    epoch_X = []\n",
    "    train_perplex = []\n",
    "    val_perplex = []\n",
    "\n",
    "    for line in f.readlines()[1:]:\n",
    "        data = line.strip().split(\",\")\n",
    "        epoch_X.append(int(data[0]))\n",
    "        train_perplex.append(2**(float(data[1])))\n",
    "        val_perplex.append(2**(float(data[3])))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "\n",
    "    ax.plot(epoch_X, train_perplex)\n",
    "    ax.plot(epoch_X, val_perplex)\n",
    "    plt.legend(['Train perplexity', 'Val perplexity'], fontsize=8, loc='upper right')\n",
    "    plt.title('Perplexities for {}'.format(name))\n",
    "    plt.savefig('{}_perplexity.jpg'.format(path+name))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True for print statements to show in terminal instead\n",
    "PRINT_TRAIN_PROGRESS_TO_TERMINAL = True \n",
    "DEBUG_MODE = False\n",
    "\n",
    "if PRINT_TRAIN_PROGRESS_TO_TERMINAL:\n",
    "    reload(sys)    \n",
    "\n",
    "if DEBUG_MODE:\n",
    "    small_samples = 200\n",
    "    nb_train_samples = small_samples\n",
    "    nb_val_samples = small_samples\n",
    "    batch_size = 128\n",
    "    NB_EPOCHS = 2\n",
    "else:\n",
    "    nb_train_samples = len(X_train_labels)\n",
    "    nb_val_samples = len(X_val_labels)\n",
    "    NB_EPOCHS = 20\n",
    "\n",
    "print 'Total training samples:', nb_train_samples\n",
    "print 'Total val samples:', nb_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # shuffling of training data True by default\n",
    "    print 'Training...'\n",
    "    history = model.fit_generator(\n",
    "        sampling_generator(nb_train_samples, batch_size), \n",
    "        steps_per_epoch=nb_train_samples/batch_size,\n",
    "        epochs=NB_EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=sampling_generator(nb_val_samples, batch_size, validation=True),\n",
    "        validation_steps=nb_val_samples/batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr, csv_logger, checkpointer, plot_history])\n",
    "\n",
    "    hist = history.history\n",
    "    plotPerplexity(PATH, MODEL_NAME)\n",
    "#     print 'Loss:', hist['loss'][0], 'and val loss:', hist['val_loss'][0]\n",
    "    print '\\nFull training history:\\n', hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Users/goldenzenith/Dropbox\\ \\(MIT\\)/6.867_saved_models/GRU_wordword/gru_512_wordword.hdf5'\n",
    "model = load_model(model_path)\n",
    "print model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_sentence_to_ohe(sentence):\n",
    "    x_label = map(lambda x: tokens_indices[x], sentence)\n",
    "    confession = np.zeros(MAX_LEN+1)\n",
    "    confession[:len(x_label)] = x_label\n",
    "    ohe_x = to_categorical(confession, num_classes=NB_CLASSES)\n",
    "    return np.expand_dims(ohe_x, axis=0)\n",
    "\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "def log_softmax(vec):\n",
    "    return vec - logsumexp(vec)\n",
    "\n",
    "def softmax(vec):\n",
    "    return np.exp(log_softmax(vec))\n",
    "\n",
    "def generate_confession(model, seed_string, seed_list, temperature, sample, thresh, confidence):\n",
    "    # nb chars to preserve\n",
    "    if CHAR_BY_CHAR:\n",
    "        orig_len = len(seed_string) \n",
    "    else:\n",
    "        orig_len = len(seed_list) \n",
    "        \n",
    "    window_str = seed_string\n",
    "    window_list = seed_list\n",
    "    final_str = seed_string \n",
    "        \n",
    "    for unit_nb in range(orig_len, MAX_LEN):\n",
    "        if CHAR_BY_CHAR:\n",
    "            x = convert_sentence_to_ohe(window_str)\n",
    "        else:\n",
    "#             print \"list here is\", window_list\n",
    "            x = convert_sentence_to_ohe(window_list)\n",
    "    \n",
    "        if sample:\n",
    "            # get next char\n",
    "            next_char_idx = min(WINDOW_SIZE, unit_nb) - 1\n",
    "        \n",
    "            # helper function to sample an index from a probability array\n",
    "            # indexing into 3D matrix -- get row of probs for single char at next_char_idx\n",
    "            preds = model.predict(x)[0, next_char_idx, :]\n",
    "            preds = np.asarray(preds).astype('float64')\n",
    "            preds = np.log(preds) / temperature\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            probas = np.random.multinomial(1, preds, 1)\n",
    "            next_token = np.argmax(probas)\n",
    "        \n",
    "            maxConf = np.max(preds)\n",
    "#             print \"max conf\",maxConf\n",
    "            if maxConf > thresh:\n",
    "                confidence += 1\n",
    "        else:\n",
    "            print x.shape\n",
    "            preds = np.squeeze(model.predict(x),axis=0) # otherwise wrapped in (1,maxlen+1,len(chars))\n",
    "            best_tokens = np.argmax(preds, axis=1)\n",
    "#             y = preds[0,:]\n",
    "#             plt.plot(y.tolist())\n",
    "#             plt.savefig(\"testttt.jpg\")\n",
    "#             plt.clf()\n",
    "\n",
    "            if unit_nb >= WINDOW_SIZE:\n",
    "                maxConf = np.max(preds[WINDOW_SIZE-1])\n",
    "            else:\n",
    "                maxConf = np.max(preds[unit_nb-1])\n",
    "                \n",
    "            if maxConf > thresh:\n",
    "                confidence += 1\n",
    "            \n",
    "            # unit_nb-1 because we want prev val in y matrix (best_tokens). In training we treat y as a shifted \n",
    "            # version of x hence offset -1 here\n",
    "            if unit_nb >= WINDOW_SIZE:\n",
    "                next_token = best_tokens[WINDOW_SIZE-1]\n",
    "            else:\n",
    "                next_token = best_tokens[unit_nb-1]\n",
    "                \n",
    "        # stop symbol\n",
    "        if next_token == STOP_INDEX:\n",
    "            if CHAR_BY_CHAR:\n",
    "                confession_list = final_str\n",
    "            else:\n",
    "                confession_list = final_str.split()\n",
    "            print \"confidence here\",confidence \n",
    "            print \"len of confession here\",len(confession_list)\n",
    "            return final_str,confidence/float(len(confession_list))\n",
    "        \n",
    "        next_char = indices_tokens[next_token]\n",
    "        \n",
    "        if CHAR_BY_CHAR:\n",
    "            if len(window_str) == WINDOW_SIZE:\n",
    "                moveConf = window_str[1:] + next_char\n",
    "                window_str = moveConf\n",
    "            else:\n",
    "                window_str += next_char\n",
    "            final_str += next_char\n",
    "        else:\n",
    "            window_list.append(next_char)\n",
    "            if len(window_list)-1 == WINDOW_SIZE:\n",
    "                moveConf = window_list[1:]\n",
    "                window_list = moveConf\n",
    "\n",
    "            if window_str != '#':\n",
    "                window_str += ' '+next_char\n",
    "                final_str += ' '+next_char\n",
    "            else:\n",
    "                window_str += next_char\n",
    "                final_str += next_char\n",
    "            \n",
    "        print \"final str:\",final_str\n",
    "        \n",
    "#     print \"\\n this\",final_str,confidence\n",
    "                \n",
    "    if CHAR_BY_CHAR:\n",
    "        confession_list = final_str\n",
    "    else:\n",
    "        confession_list = final_str.split()\n",
    "    print \"confidence there\",confidence\n",
    "    print \"len of confession there\",len(confession_list)\n",
    "    return final_str,confidence/float(len(confession_list))\n",
    "\n",
    "\n",
    "# Run and place into .txt file\n",
    "ARG_MAX = True\n",
    "if ARG_MAX:\n",
    "    filename = MODEL_NAME+'_preds.txt'\n",
    "    sample_bool = False\n",
    "else:\n",
    "    filename = MODEL_NAME+'_temp_0_5_preds.txt'\n",
    "    sample_bool = True\n",
    "    \n",
    "# seed_strings_cc=[\"#\", \"#1\", \"#12\", \"#124\", \"#1246\", \"#12465\", \"#9999\", \n",
    "#              \"#1 Hi\", \"#12 Hi\", \"#124 Hi\", \"#1246 Hi\", \"#1293 Hi m\",\n",
    "#              \"#2568 ML\", \"#3476 Hixz\", \"#1321 I'm\", \"#2346 :)\", \"#5876 7:30am\",\n",
    "#              \"#9235 6.01\", \"#1246 6.867\"]\n",
    "seed_strings_cc=[\"#\", \"#3476\", \"#3476 i\",\n",
    "             \"#3476 i'm\", \"#3476 :)\", \n",
    "             \"#3476 7:30am\",\n",
    "             \"#3476 6.01\"]\n",
    "seed_strings_ww=[\"#\", \"# CUSTOM_NUMBER\", \"# CUSTOM_NUMBER i\",\n",
    "             \"# CUSTOM_NUMBER i ' m\", \"# CUSTOM_NUMBER :)\", \n",
    "             \"# CUSTOM_NUMBER CUSTOM_NUMBER :3 CUSTOM_NUMBER am\",\n",
    "             \"# CUSTOM_NUMBER 6.01\"]\n",
    "conf_list = []\n",
    "confidence = 0\n",
    "\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    print \"Writing to\",filename\n",
    "    if CHAR_BY_CHAR:\n",
    "        seed_list = seed_strings_cc\n",
    "    else:\n",
    "        seed_list = seed_strings_ww\n",
    "        \n",
    "    for seed_string in seed_list:\n",
    "        print \"seed_string:\",seed_string\n",
    "        seed_list = seed_string.split()\n",
    "        #set to true for temperature sampling version\n",
    "        confession, confidence = generate_confession(model, seed_string, seed_list, 0.5, sample_bool, 0.8, 0) \n",
    "        print \"confession:\",confession\n",
    "        print \"confidence:\", confidence\n",
    "        conf_list.append(confidence)\n",
    "        \n",
    "#         if len(conf_list) >1:\n",
    "#             break\n",
    "        f.write(confession)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    avg_conf = sum(conf_list)/len(conf_list)\n",
    "    f.write(\"confidence list: \"+ str(conf_list))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"average confidence: \"+ str(avg_conf))\n",
    "        \n",
    "print \"confidence for test set is:\",conf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
