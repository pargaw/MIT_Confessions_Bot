{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.callbacks import EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Activation, Dropout, LSTM, TimeDistributed  \n",
      "from keras.utils import to_categorical\n",
      "from sklearn.model_selection import train_test_split\n",
      "from time import sleep\n",
      "import csv\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named keras.callbacks",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-cd3c16cd4114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named keras.callbacks"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# change to False if you want to train on datasets \n",
      "# where we don't use CUSTOM_NAME token for tagged Facebook names\n",
      "custom_names = True\n",
      "\n",
      "if custom_names:\n",
      "    comments_filepath = \"csv_data/CUSTOM_NAMES/custom_name_token_comments.csv\"\n",
      "else:\n",
      "    comments_filepath = \"csv_data/FILTERED_NAMES/filtered_names_comments.csv\"\n",
      "\n",
      "status_filepath = \"csv_data/beaverconfessions_facebook_statuses.csv\"\n",
      "        \n",
      "stop_symbol = \"`\"\n",
      "\n",
      "def load_dataset(filepath, dictionary, datatype):\n",
      "    dt = \"{}_message\".format(datatype)\n",
      "    \n",
      "    with open(filepath, \"rU\") as csvfile:\n",
      "        reader = csv.DictReader(csvfile)\n",
      "        sentences = []\n",
      "        \n",
      "        for status in reader:\n",
      "            if dt not in status:\n",
      "                # 2 rows are being read at a time for the statuses csv for some reason...\n",
      "                # handled with monkey patching below\n",
      "                msg1, msg2 = status.items()[1]\n",
      "\n",
      "                for char in msg1:\n",
      "                    dictionary.add(char)\n",
      "                for char in msg2:\n",
      "                    dictionary.add(char)\n",
      "                \n",
      "                # add stop symbol to end before text->int conversion\n",
      "                msg1 += stop_symbol\n",
      "                msg2 += stop_symbol\n",
      "                sentences.append(msg1)\n",
      "                sentences.append(msg2)\n",
      "            else:\n",
      "                msg = status[dt]\n",
      "                for char in msg:\n",
      "                    dictionary.add(char)\n",
      "                msg += stop_symbol\n",
      "                sentences.append(msg)\n",
      "                    \n",
      "    return dictionary, sentences\n",
      "    \n",
      "dictionary, comments = load_dataset(comments_filepath, set([]), \"comment\")\n",
      "dictionary, statuses = load_dataset(status_filepath, dictionary, \"status\")\n",
      "sentences = []\n",
      "sentences.extend(comments)\n",
      "sentences.extend(statuses)\n",
      "sentences_arr = np.array(sentences)\n",
      "\n",
      "# shuffle statuses and comments\n",
      "random_permutation = np.random.permutation(len(sentences))\n",
      "sentences_arr = sentences_arr[random_permutation]\n",
      "\n",
      "print 'Sample sentences:'\n",
      "print '-', sentences_arr[0]\n",
      "print '-', sentences_arr[1]\n",
      "print\n",
      "\n",
      "# create vocabulary of characters found in data\n",
      "chars = sorted(list(dictionary))\n",
      "padding_symbol = \"{\"\n",
      "chars.insert(0, stop_symbol) # index 1\n",
      "chars.insert(0, padding_symbol) # index 0 \n",
      "# print chars\n",
      "\n",
      "# print('total chars:', len(chars))\n",
      "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
      "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
      "nb_classes = len(char_indices)\n",
      "print char_indices\n",
      "\n",
      "\n",
      "# TODO separate model_related values and preprocessing code\n",
      "maxlen = 280\n",
      "window_size = 10 # TODO try smaller and bigger window sizes\n",
      "step = 1 # char by char\n",
      "\n",
      "sub_sentences = []\n",
      "next_chars = []\n",
      "for sentence in sentences:\n",
      "    for i in range(0, maxlen - window_size+1, step):\n",
      "        sub_sentences.append(sentence[i : i+window_size])\n",
      "        next_chars.append(sentence[i+1 : i+1+window_size])\n",
      "\n",
      "nb_samples = len(sub_sentences)\n",
      "\n",
      "# any values not filled in represent padding \n",
      "# extra 1 represents space for the stop symbol\n",
      "X_labels = np.zeros((nb_samples, maxlen+1))\n",
      "y_labels = np.zeros((nb_samples, maxlen+1))\n",
      "\n",
      "for sample_nb in range(nb_samples):\n",
      "    x_label = map(lambda x: char_indices[x], sub_sentences[sample_nb])\n",
      "    y_label = map(lambda x: char_indices[x], next_chars[sample_nb])\n",
      "\n",
      "    X_labels[sample_nb][:len(x_label)] = x_label\n",
      "    y_labels[sample_nb][:len(y_label)] = y_label\n",
      "\n",
      "# print X_labels, y_labels\n",
      "\n",
      "print('# training samples:', nb_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make use of subsentences, next_chars, which are our words (semantic, not encoded yet)\n",
      "\n",
      "def validate_data(X, y):\n",
      "    # assert at least one nonzero value in each OHE sample\n",
      "    assert(False not in np.any(X>0, axis=1))\n",
      "    assert(False not in np.any(y>0, axis=1))\n",
      "\n",
      "def sampling_generator(epoch_size, batch_size, validation=False, sample_weights=False):  \n",
      "    \"\"\"\n",
      "    Takes in labels of int values, e.g. [1. 2. 5. 9. 1. 0. 0. ...]\n",
      "    These labels are already padded with 0s to a predetermined maxlen.\n",
      "    We convert each sample X and y to one hot versions, and create batches.\n",
      "    Batch generation is necessary to avoid MemoryError.\n",
      "    \"\"\"\n",
      "    \n",
      "    if validation:\n",
      "        X_labels = X_train_labels\n",
      "        y_labels = y_train_labels\n",
      "    else:\n",
      "        X_labels = X_val_labels\n",
      "        y_labels = y_val_labels\n",
      "    \n",
      "    while True:\n",
      "        # hand off data in batches\n",
      "        for i in range(2): #int(epoch_size/batch_size)\n",
      "            start = i * batch_size\n",
      "            end = min(start + batch_size, epoch_size)\n",
      "            true_batch_size = end - start\n",
      "            \n",
      "            # fresh batch\n",
      "            sample_X = []\n",
      "            sample_Y = []\n",
      "\n",
      "            # one hot encode\n",
      "            for i in range(true_batch_size):                \n",
      "                ohe_x = to_categorical(X_labels[i], num_classes=nb_classes)\n",
      "                ohe_y = to_categorical(y_labels[i], num_classes=nb_classes)\n",
      "                \n",
      "                # ensure no 0 zeros in data - otherwise training NaNs out\n",
      "                # validate_data(ohe_x, ohe_y)\n",
      "                \n",
      "                # add dim of size 1 to front of np arrays to allow for vstacking\n",
      "                ohe_x = np.expand_dims(ohe_x, axis=0)\n",
      "                ohe_y = np.expand_dims(ohe_y, axis=0)\n",
      "                                \n",
      "                # build the batch\n",
      "                if len(sample_X) == 0:\n",
      "                    sample_X = ohe_x\n",
      "                    sample_Y = ohe_y\n",
      "                else:\n",
      "                    sample_X = np.vstack((sample_X, ohe_x))\n",
      "                    sample_Y = np.vstack((sample_Y, ohe_y))\n",
      "                        \n",
      "            if sample_weights:\n",
      "                # TODO fill in with reactions if so desired :)\n",
      "                sample_weights = []\n",
      "                yield (sample_x, sample_y, sample_weights)\n",
      "            \n",
      "            yield (sample_X, sample_Y)\n",
      "\n",
      "# print next(sampling_generator())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build the model: 2 stacked LSTM\n",
      "print('Building model...')\n",
      "model = Sequential()\n",
      "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen+1, nb_classes)))\n",
      "model.add(LSTM(512, return_sequences=True))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(TimeDistributed(Dense(nb_classes)))\n",
      "model.add(Activation('softmax'))\n",
      "\n",
      "model.compile(loss='categorical_crossentropy', optimizer='rmsprop') # TODO switch to Adam for more stability?\n",
      "print ('Model is made!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split into train and val data\n",
      "X_train_labels, X_val_labels, y_train_labels, y_val_labels = train_test_split(X_labels, y_labels, test_size=0.3)\n",
      "nb_train_samples = len(X_train_labels)\n",
      "nb_val_samples = len(X_val_labels)\n",
      "batch_size = 128\n",
      "\n",
      "# callbacks\n",
      "model_name = 'lstm_512_simple'\n",
      "csv_logger = CSVLogger('{}.log'.format(model_name))\n",
      "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
      "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
      "                                              verbose=1, epsilon=1e-4, min_lr=1E-6) \n",
      "\n",
      "# train\n",
      "history = model.fit_generator(\n",
      "    sampling_generator(nb_train_samples, batch_size), \n",
      "    steps_per_epoch=2,#nb_train_samples/batch_size,\n",
      "    epochs=1, #10,\n",
      "    verbose=1,\n",
      "    validation_data=sampling_generator(nb_val_samples, batch_size, validation=True),\n",
      "    validation_steps=2,#nb_val_samples/batch_size,\n",
      "    callbacks=[early_stopping, reduce_lr, csv_logger])\n",
      "    \n",
      "sleep(0.1) # https://github.com/fchollet/keras/issues/2110 \n",
      "\n",
      "hist = history.history\n",
      "print 'Loss and val_loss is', hist['loss'][0], hist['val_loss'][0]\n",
      "print history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_size = 100\n",
      "for iteration in range(1, 6):\n",
      "    print '-' * 50\n",
      "    print 'Iteration', iteration\n",
      "    history = model.fit(X[:sample_size], y[:sample_size], batch_size=128, nb_epoch=1,verbose=1)\n",
      "    sleep(0.1) # https://github.com/fchollet/keras/issues/2110\n",
      "    \n",
      "    # saving models at the following iterations -- uncomment it if you want tos save weights and load it later\n",
      "    #if iteration==1 or iteration==3 or iteration==5 or iteration==10 or iteration==20 or iteration==30 or iteration==50 or iteration==60 :\n",
      "    #    model.save_weights('Karpathy_LSTM_weights_'+str(iteration)+'.h5', overwrite=True)\n",
      "    #start_index = random.randint(0, len(text) - maxlen - 1)\n",
      "\n",
      "    #sys.stdout.flush()\n",
      "    print 'Loss is', history.history['loss'][0]\n",
      "    print history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seed_string=\"#9 I\"\n",
      "original_len = len(seed_string)\n",
      "seed_string = pad_sentence(seed_string, stop=False)\n",
      "print (\"seed string -->\", seed_string), len(seed_string)\n",
      "print ('The generated text is')\n",
      "prediction = \"\"\n",
      "for i in range(maxlen):\n",
      "    x=np.zeros((1, len(seed_string), len(chars)))\n",
      "    for t, char in enumerate(seed_string):\n",
      "        x[0, t, char_indices[char]] = 1.\n",
      "    preds = model.predict(x, verbose=0)[0]\n",
      "    print preds\n",
      "    next_index=np.argmax(preds[len(seed_string)-1]) # last char of window\n",
      "    print next_index \n",
      "    next_char = indices_char[next_index]\n",
      "    if original_len+1 < len(seed_string):\n",
      "        seed_string = seed_string[:original_len] + next_char + seed_string[original_len+1:]\n",
      "    else:\n",
      "        seed_string = seed_string[:original_len] + next_char\n",
      "    original_len += 1\n",
      "    print seed_string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}