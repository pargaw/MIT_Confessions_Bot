{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBALS AND INITIAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from emot import emo_unicode\n",
    "from keras.callbacks import Callback, EarlyStopping, CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, TimeDistributed,Masking \n",
    "from keras.models import load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize #, wordpunct_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import sleep\n",
    "import csv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "# settings\n",
    "CHAR_BY_CHAR = False # True if doing character-by-character training\n",
    "CUSTOM_NAMES = True # set as True to use CUSTOM_NAME token (and not empty str) for tagged Facebook names\n",
    "MODEL_NAME = 'lstm_512_wordword' # name models descriptively, e.g. with param sizes like lstm_512_hidden\n",
    "PRINT_TRAIN_PROGRESS_TO_TERMINAL = True # set True to see losses change in real-time in terminal \n",
    "\n",
    "# parameters\n",
    "# MAX_LEN is defined later\n",
    "HIDDEN_UNITS = 512\n",
    "NB_EPOCHS = 2#20\n",
    "STEP_SIZE = 1\n",
    "WINDOW_SIZE = 10 \n",
    "LSTM_MODEL = True\n",
    " \n",
    "# paths with data based on filtering done in adapted version of Facebook post scraping\n",
    "STATUS_FILEPATH = \"csv_data/beaverconfessions_facebook_statuses.csv\"\n",
    "\n",
    "if CUSTOM_NAMES:\n",
    "    COMMENTS_FILEPATH = \"csv_data/CUSTOM_NAMES/custom_name_token_comments.csv\"\n",
    "else:\n",
    "    COMMENTS_FILEPATH = \"csv_data/FILTERED_NAMES/filtered_names_comments.csv\"\n",
    "\n",
    "# symbols\n",
    "CUSTOM_NAME = 'CUSTOM_NAME'\n",
    "CUSTOM_NUMBER = 'CUSTOM_NUMBER'\n",
    "PADDING_SYMBOL = \"{\"\n",
    "STOP_SYMBOL = \"`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING (TOKENIZATION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaped punctuation: \\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~\n",
      "\n",
      "Test case: ['custom_name', '6.867', 'test', '.', '@', 'CUSTOM_NUMBER', ':3', 'CUSTOM_NUMBER', 'pm', 'in', 'CUSTOM_NUMBER', '-', 'CUSTOM_NUMBER', '.', 'the', 'profs', ',', 'they', \"'\", 're', 'coming', 'in', 'CUSTOM_NUMBER', '..', 'CUSTOM_NUMBER', '..', 'CUSTOM_NUMBER', '.', 'lets', 'dooo', 'this', '!!!', '>;)', '`']\n"
     ]
    }
   ],
   "source": [
    "escaped_punctuation = re.escape(string.punctuation)\n",
    "print 'Escaped punctuation:', escaped_punctuation\n",
    "\n",
    "# insert an OR pipe before each punctuation mark\n",
    "xor_punctuation = '|'.join('{}{}+'.format(escaped_punctuation[x], escaped_punctuation[x+1]) for x in range(0, len(escaped_punctuation), 2))\n",
    "# print 'Delimited punctuation:', xor_punctuation\n",
    "\n",
    "# build regex with variable (order matters!)\n",
    "custom_name = CUSTOM_NAME.lower()\n",
    "course_number = '\\d+\\.\\d{2,3}'\n",
    "multiple_numbers = '\\d+'\n",
    "emoticon_pattern = '|'.join(emoticon for emoticon in emo_unicode.EMOTICONS)\n",
    "space = '\\s'\n",
    "regex_expr = r'(' + '|'.join([custom_name, course_number, multiple_numbers, emoticon_pattern, xor_punctuation, space]) + r')'\n",
    "# print '\\nRegex expression:', regex_expr\n",
    "\n",
    "def replace_digit_with_token(string): \n",
    "    return CUSTOM_NUMBER if string.isdigit() else string\n",
    "\n",
    "def tokenize_str(string, replace_digits=True):\n",
    "    \"\"\"\n",
    "    NLTK tokenizers (word_tokenize, wordpunct_tokenize) are insufficient, \n",
    "    as emoticons and course #s, e.g. 6.111, are important to our dataset.\n",
    "    \n",
    "    This is a custom tokenizer to retain such items in the vocab,\n",
    "        but split up other words containing numbers/punctuation, e.g. 3pm.\n",
    "        \n",
    "    Note that these texts are lowercased by default prior to tokenization.\n",
    "    \"\"\"\n",
    "    if CHAR_BY_CHAR:\n",
    "        tokens = list(string)\n",
    "        \n",
    "    else:\n",
    "        tokens = re.split(regex_expr, string.lower())\n",
    "\n",
    "        # filter out spaces\n",
    "        tokens = [token for token in tokens if token not in [\"\", \" \"]]\n",
    "\n",
    "        # tokens with *just* digits are mapped to CUSTOM_NUMBER by default\n",
    "        if replace_digits:\n",
    "            tokens = map(lambda x: replace_digit_with_token(x), tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# attempt to catch major cases in our dataset\n",
    "sentence = 'CUSTOM_NAME 6.867test. @7:30pm in 54-100. The profs, they\\'re coming in 3..2..1. LETS DOOO THIS!!! >;)' + STOP_SYMBOL\n",
    "print '\\nTest case:', tokenize_str(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max # of tokens in a sentence: 80\n",
      "['#', 'CUSTOM_NUMBER', 'i', 'had', 'feelings', 'for', 'you', 'last', 'year', ',', 'but', 'i', 'was', 'only', 'a', 'frosh', 'then', '.', 'now', ',', 'you', \"'\", 're', 'a', 'senior', ',', 'and', 'i', 'don', \"'\", 't', 'know', 'if', 'you', \"'\", 'd', 'be', 'willing', 'to', 'make', 'the', 'emotional', 'investment', '.', 'i', 'know', 'i', \"'\", 'd', 'be', 'willing', 'to', '.', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'i', 'just', 'looked', 'at', 'another', 'junior', \"'\", 's', 'resume', 'and', 'got', 'legitimately', 'intimidated', '.', 'we', 'both', 'did', 'this', 'one', 'thing', 'that', \"'\", 's', 'the', '\"', 'star', 'item', '\"', 'on', 'my', 'resume', 'and', 'it', \"'\", 's', 'literally', 'left', 'off', 'hers', 'because', 'she', 'didn', \"'\", 't', 'have', 'enough', 'room', 'with', 'all', 'of', 'her', 'other', 'crazy', 'stuff', '.', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'to', 'the', 'quad', 'of', 'girls', 'that', 'live', 'right', 'next', 'to', 'me', ':', 'i', 'really', 'wish', 'you', 'knew', 'that', 'my', 'walls', 'aren', \"'\", 't', 'as', 'soundproof', 'as', 'you', 'think', '.', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'i', 'hooked', 'up', 'with', 'a', 'junior', 'guy', 'in', 'a', 'fraternity', 'just', 'because', 'my', 'friend', 'did', 'two', 'days', 'earlier', 'and', 'i', 'didn', \"'\", 't', 'want', 'her', 'to', 'think', 'she', 'was', 'better', 'than', 'me', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'whenever', 'my', 'friend', 'gets', 'really', 'fucked', 'up', 'she', 'forgets', 'english', 'and', 'starts', 'babbling', 'french', 'like', 'an', 'idiot', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'some', 'of', 'the', 'upperclassmen', 'in', 'our', 'fraternity', 'really', 'struggle', 'at', 'getting', 'with', 'girls', 'apparently', ',', 'because', 'they', 'started', 'throwing', 'parties', 'that', 'only', 'freshman', 'girls', '(', 'not', 'even', 'our', 'pledges', ')', 'could', 'get', 'into', '.', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'i', 'just', 'realized', '-', 'mit', 'doesn', \"'\", 't', 'let', 'its', 'students', 'compete', 'for', 'grades', '.', 'so', 'instead', ',', 'we', 'compete', 'to', 'see', 'how', 'miserable', 'we', 'can', 'make', 'ourselves', '.', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'thank', 'you', 'senior', 'haus', 'residents', '!', 'thanks', 'to', 'you', 'guys', ',', 'i', \"'\", 'm', 'living', 'in', 'an', 'amazing', 'grad', 'dorm', '!', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'i', \"'\", 'm', 'more', 'proud', 'of', 'how', 'my', 'bitcoin', 'value', 'has', 'grown', 'than', 'how', 'i', \"'\", 've', 'grown', 'in', 'the', 'last', 'CUSTOM_NUMBER', 'years', '.', '`']\n",
      "['#', 'CUSTOM_NUMBER', 'it', 'gets', 'so', 'hot', 'in', 'my', 'dorm', '.', 'sometimes', ',', 'i', 'just', 'want', 'take', 'off', 'all', 'my', 'clothes', 'and', 'have', 'all', 'the', 'boys', 'in', 'my', 'lick', 'my', 'nipples', 'to', 'cool', 'me', 'down', '.', 'these', 'freshman', 'boys', 'are', 'so', 'cute', '.', 'as', 'a', 'senior', ',', 'i', 'think', 'it', 'would', 'be', 'my', 'responsibility', 'to', 'teach', 'them', 'how', 'to', 'pleasure', 'the', 'female', 'body', '.', '`']\n",
      "\n",
      "The padding index is: 7481\n",
      "\n",
      "The stop index is: 138\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_and_vocabulary(filepath, vocabulary, datatype, max_nb_tokens):\n",
    "    \"\"\"\n",
    "    Read individual sentences into memory, and generate vocabulary.\n",
    "    \n",
    "    If CHAR_BY_CHAR, the vocabulary will hold single characters, \n",
    "        e.g. a-zA-Z and punctuation.\n",
    "    Else, it will contain whole words and Unicode 'phrases',\n",
    "        e.g. :\\'(, as split by our custom tokenizer.\n",
    "    \"\"\"\n",
    "    dt = \"{}_message\".format(datatype)\n",
    "    \n",
    "    with open(filepath, \"rU\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        tokens = []\n",
    "        \n",
    "        for status in reader:\n",
    "            # we distinguish between comments and statuses with header cols in the CSVs\n",
    "            if dt not in status:\n",
    "                # 2 rows are read at once from STATUS_FILEPATH for some reason...\n",
    "                # handled with monkey patching \n",
    "                msg1, msg2 = status.items()[1]\n",
    "                \n",
    "                # append stop symbol so model has explicit marker for ending\n",
    "                msg1 += STOP_SYMBOL\n",
    "                msg2 += STOP_SYMBOL\n",
    "                \n",
    "                # we define elts as the unit we are training on, e.g. char vs. word \n",
    "                elts_in_sentence1 = tokenize_str(msg1)\n",
    "                elts_in_sentence2 = tokenize_str(msg2)\n",
    "                \n",
    "                max_nb_tokens = max(max_nb_tokens, max(len(elts_in_sentence1), len(elts_in_sentence2)))\n",
    "                 \n",
    "                for elt in elts_in_sentence1:\n",
    "                    vocabulary.add(elt)\n",
    "                for elt in elts_in_sentence2:\n",
    "                    vocabulary.add(elt) \n",
    "                    \n",
    "                # build list of tokens in local memory\n",
    "                tokens.append(elts_in_sentence1)\n",
    "                tokens.append(elts_in_sentence2)\n",
    "            else:\n",
    "                msg = status[dt]\n",
    "                msg += STOP_SYMBOL \n",
    "                elts_in_sentence = tokenize_str(msg)\n",
    "                max_nb_tokens = max(max_nb_tokens, len(elts_in_sentence))\n",
    "                    \n",
    "                for elt in elts_in_sentence:\n",
    "                    vocabulary.add(elt)\n",
    "                    \n",
    "                tokens.append(elts_in_sentence)\n",
    "                    \n",
    "    return vocabulary, tokens, max_nb_tokens\n",
    "    \n",
    "# load comments and statuses separately at first\n",
    "max_nb_tokens = 0\n",
    "vocabulary, comment_tokens, max_nb_tokens = load_dataset_and_vocabulary(COMMENTS_FILEPATH, set([]), \"comment\", max_nb_tokens)\n",
    "vocabulary, status_tokens, max_nb_tokens = load_dataset_and_vocabulary(STATUS_FILEPATH, vocabulary, \"status\", max_nb_tokens)\n",
    "MAX_LEN = max_nb_tokens\n",
    "\n",
    "print 'Max # of tokens in a sentence:', MAX_LEN\n",
    "# print '\\nSample of 10 processed sentences:'\n",
    "for status in status_tokens[:10]:\n",
    "    print status\n",
    "\n",
    "# create vocabulary of characters found in data\n",
    "if not CHAR_BY_CHAR:\n",
    "    vocabulary.add(CUSTOM_NAME)\n",
    "    vocabulary.add(CUSTOM_NUMBER)\n",
    "    \n",
    "# these symbols did not exist in the original training set,\n",
    "# so we can safely add them \n",
    "# NOTE: changed from insert(0,_), since we were adding duplicates\n",
    "vocabulary.add(STOP_SYMBOL)\n",
    "vocabulary.add(PADDING_SYMBOL)\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "\n",
    "NB_CLASSES = len(vocabulary)\n",
    "\n",
    "tokens_indices = dict((t, i) for i, t in enumerate(vocabulary))\n",
    "indices_tokens = {v: k for k, v in tokens_indices.iteritems()}\n",
    "PADDING_INDEX = tokens_indices[PADDING_SYMBOL]\n",
    "STOP_INDEX = tokens_indices[STOP_SYMBOL]\n",
    "print '\\nThe padding index is:', PADDING_INDEX\n",
    "print '\\nThe stop index is:', STOP_INDEX\n",
    "\n",
    "input_type = \"character by character\" if CHAR_BY_CHAR else \"word by word\"\n",
    "# print \"For\", input_type, \", we have this vocabulary:\", vocabulary, \"\\n of size\", NB_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD TRAINING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized sentence:\n",
      "['#', 'CUSTOM_NUMBER', 'something', 'something', 'it', \"'\", 's', 'what', 'you', 'make', 'of', 'the', 'experience', ',', 'maybe', 'you', 'just', 'grew', 'and', 'learned', 'more', 'from', 'it', 'than', 'them', '`']\n",
      "\n",
      "Sample X: ['#', 'CUSTOM_NUMBER', 'something', 'something', 'it', \"'\", 's', 'what', 'you', 'make']\n",
      "Sample Y: ['CUSTOM_NUMBER', 'something', 'something', 'it', \"'\", 's', 'what', 'you', 'make', 'of']\n",
      "\n",
      "Sample X labels:\n",
      "[[    8.   130.  6166.  6166.  3562.    13.  5654.  7270.  7455.  4046.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.]]\n",
      "Sample Y labels:\n",
      "[[  130.  6166.  6166.  3562.    13.  5654.  7270.  7455.  4046.  4571.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.]]\n",
      "\n",
      "Total # of training samples: 64451\n"
     ]
    }
   ],
   "source": [
    "# merge comments and statuses\n",
    "tokens = []\n",
    "tokens.extend(comment_tokens)\n",
    "tokens.extend(status_tokens)\n",
    "tokens = np.array(tokens)\n",
    "\n",
    "# shuffle dataset (commented out b/c we shuffle during training anyway)\n",
    "# random_permutation = np.random.permutation(len(tokens))\n",
    "# tokens = tokens_arr[random_permutation]\n",
    "\n",
    "print 'Sample tokenized sentence:'\n",
    "# print tokens_arr\n",
    "print tokens[0]\n",
    "print\n",
    "\n",
    "def generate_X_and_Y(sentences):\n",
    "    \"\"\"\n",
    "    X = sub_sentences, Y = next_sub_sentences\n",
    "    Y is simply X shifted over by step_size.\n",
    "    We want sub sentences per sentence to create multiple samples.\n",
    "    \"\"\"\n",
    "    sub_sentences = []\n",
    "    next_sub_sentences = []\n",
    "    \n",
    "    for sentence_nb, sentence in enumerate(sentences):\n",
    "        for i in range(0, len(sentence) - WINDOW_SIZE, STEP_SIZE):\n",
    "            sub_sentences.append(sentence[i : i+WINDOW_SIZE])\n",
    "            next_sub_sentences.append(sentence[(i+STEP_SIZE) : (i+STEP_SIZE)+WINDOW_SIZE])\n",
    "    \n",
    "    return sub_sentences, next_sub_sentences\n",
    "\n",
    "X, Y = generate_X_and_Y(tokens)\n",
    "nb_samples = len(X)\n",
    "print 'Sample X:', X[0]\n",
    "print 'Sample Y:', Y[0]\n",
    "print \n",
    "\n",
    "def convert_tokens_to_int(X, Y, nb_samples):\n",
    "    \"\"\"\n",
    "    Convert token to integer representations.\n",
    "    \n",
    "    Populate zero-filled label matrices with ints,\n",
    "        so end result is padded and ready for training.\n",
    "    \"\"\"\n",
    "    # any values not filled in later represent padding \n",
    "    # extra 1 represents space for the stop symbol\n",
    "    input_shape = (nb_samples, MAX_LEN+1)\n",
    "    X_labels = np.zeros(input_shape)\n",
    "    y_labels = np.zeros(input_shape)\n",
    "\n",
    "    for sample_nb in range(nb_samples):\n",
    "        x_label = map(lambda x: tokens_indices[x], X[sample_nb]) \n",
    "        y_label = map(lambda x: tokens_indices[x], Y[sample_nb])\n",
    "\n",
    "        X_labels[sample_nb][:len(x_label)] = x_label\n",
    "        y_labels[sample_nb][:len(y_label)] = y_label\n",
    "    \n",
    "    return X_labels, y_labels\n",
    "\n",
    "X_labels, y_labels = convert_tokens_to_int(X, Y, nb_samples)\n",
    "\n",
    "print 'Sample X labels:\\n', X_labels[:1]\n",
    "print 'Sample Y labels:\\n', y_labels[:1]\n",
    "print '\\nTotal # of training samples:', nb_samples\n",
    "\n",
    "# split data into train and val sets\n",
    "X_train_labels, X_val_labels, y_train_labels, y_val_labels = train_test_split(X_labels, y_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PRINT_TRAIN_PROGRESS_TO_TERMINAL:\n",
    "    reload(sys)\n",
    "\n",
    "def validate_data(X, y):\n",
    "    # assert at least one nonzero value in each OHE sample\n",
    "    assert(False not in np.any(X>0, axis=1))\n",
    "    assert(False not in np.any(y>0, axis=1))\n",
    "\n",
    "def sampling_generator(epoch_size, batch_size, validation=False):  \n",
    "    \"\"\"\n",
    "    Takes in labels of int values, e.g. [1. 2. 5. 9. 1. 0. 0. ...]\n",
    "    These labels are already padded with 0s to a predetermined maxlen.\n",
    "    We convert each sample X and y to one hot versions, and create batches.\n",
    "    Batch generation is necessary to avoid MemoryError.\n",
    "    \"\"\"\n",
    "    if validation:\n",
    "        X_labels = X_val_labels \n",
    "        y_labels = y_val_labels \n",
    "    else:\n",
    "        X_labels = X_train_labels\n",
    "        y_labels = y_train_labels\n",
    "    \n",
    "    while True:\n",
    "        # hand off data in batches\n",
    "        for i in range(int(epoch_size/batch_size)):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, epoch_size)\n",
    "            true_batch_size = end - start\n",
    "\n",
    "            # fresh batch\n",
    "            batch_X = []\n",
    "            batch_Y = []\n",
    "            batch_shape = (true_batch_size, MAX_LEN+1, NB_CLASSES)\n",
    "            \n",
    "            X_label_sample = X_labels[start:end]\n",
    "\n",
    "            # one hot encode\n",
    "            batch_X = to_categorical(X_label_sample, num_classes=NB_CLASSES).reshape(batch_shape)\n",
    "            batch_Y = to_categorical(y_labels[start:end], num_classes=NB_CLASSES).reshape(batch_shape)\n",
    "            \n",
    "            total_nb_one_hots = X_label_sample.size\n",
    "            sample_weights = np.ones((total_nb_one_hots, 1))\n",
    "            \n",
    "            temp_reshaped_Xs = np.ravel(X_label_sample).reshape(-1)\n",
    "            padded_positions = np.where(temp_reshaped_Xs == PADDING_INDEX)\n",
    "            sample_weights[padded_positions] = 0 # 28100 x 1\n",
    "            sample_weights = sample_weights.reshape(true_batch_size, MAX_LEN+1)\n",
    "            \n",
    "#             if sample_weights:\n",
    "#                 # TODO fill in with reactions data for later experimentation\n",
    "#                 sample_weights = ...\n",
    "            \n",
    "            yield (batch_X, batch_Y, sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "Model is made!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 81, 512)           16377856  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 81, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 81, 7484)          3839292   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 81, 7484)          0         \n",
      "=================================================================\n",
      "Total params: 22,316,348\n",
      "Trainable params: 22,316,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    if LSTM_MODEL:\n",
    "        print('Building LSTM model...')\n",
    "        model = Sequential()\n",
    "        # model.add(Masking(input_shape=(MAX_LEN+1,1)))\n",
    "        model.add(LSTM(512, return_sequences=True, input_shape=(MAX_LEN+1,NB_CLASSES)))\n",
    "        model.add(LSTM(512, return_sequences=True))\n",
    "        # model.add(Dropout(0.2))\n",
    "        model.add(TimeDistributed(Dense(NB_CLASSES)))\n",
    "        model.add(Activation('softmax'))\n",
    "        # if we want to apply sample weights to metrics, we need to specify weighted_metrics=[list of metrics]\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.0), sample_weight_mode=\"temporal\")   \n",
    "    else:\n",
    "        print('Building GRU model...')\n",
    "        model = Sequential()\n",
    "        model.add(GRU(512, return_sequences=True, input_shape=(MAX_LEN+1, NB_CLASSES)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(512, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.0), sample_weight_mode=\"temporal\")\n",
    "\n",
    "    print ('Model is made!')\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALLBACKS (bells + whistles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlotHistory(Callback):\n",
    "    def __init__(self, run_name):\n",
    "        self.run_name = run_name \n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # create loss and perplexity plot\n",
    "        loss_handles = []\n",
    "        train_perp = []\n",
    "        val_perp = []\n",
    "        for key in self.history:           \n",
    "            if key != \"lr\":\n",
    "                l, = plt.plot(self.history[key], label=key)\n",
    "                loss_handles.append(l)\n",
    "\n",
    "        plt.title('Losses and metrics for {}'.format(self.run_name))    \n",
    "        plt.ylabel('loss')\n",
    "        plt.yscale('symlog')\n",
    "        plt.legend([\"Training Loss\",\"Validation Loss\"], fontsize=8, loc='upper right')          \n",
    "        plt.savefig('{}_plot.jpg'.format(self.run_name))        \n",
    "        plt.clf()\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='{}.hdf5'.format(MODEL_NAME), verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('{}.log'.format(MODEL_NAME))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, verbose=1)\n",
    "plot_history = PlotHistory(MODEL_NAME)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, epsilon=1e-4, min_lr=1E-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotPerplexity(model):\n",
    "    f = open(model+'.log','r')\n",
    "    epoch_X = []\n",
    "    train_perplex = []\n",
    "    val_perplex = []\n",
    "\n",
    "    for line in f.readlines()[1:]:\n",
    "        data = line.strip().split(\",\")\n",
    "        epoch_X.append(int(data[0]))\n",
    "        train_perplex.append(2**(float(data[1])))\n",
    "        val_perplex.append(2**(float(data[3])))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "\n",
    "    ax.plot(epoch_X, train_perplex)\n",
    "    ax.plot(epoch_X, val_perplex)\n",
    "    plt.legend(['Training perplexity', 'Validation perplexity'], fontsize=8, loc='upper right')\n",
    "    plt.title('Perplexities for {}'.format(model))\n",
    "    plt.savefig('{}_perplexity.jpg'.format(model))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN / LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 200\n",
      "Total val samples: 200\n",
      "Training...\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/data_utils.py\", line 630, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"<ipython-input-5-c09ec96564ab>\", line 39, in sampling_generator\n",
      "    batch_Y = to_categorical(y_labels[start:end], num_classes=NB_CLASSES).reshape(batch_shape)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/np_utils.py\", line 28, in to_categorical\n",
      "    categorical = np.zeros((n, num_classes))\n",
      "MemoryError\n",
      "\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/data_utils.py\", line 630, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"<ipython-input-5-c09ec96564ab>\", line 38, in sampling_generator\n",
      "    batch_X = to_categorical(X_label_sample, num_classes=NB_CLASSES).reshape(batch_shape)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/np_utils.py\", line 28, in to_categorical\n",
      "    categorical = np.zeros((n, num_classes))\n",
      "MemoryError\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a8d6e4f05f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Full training history:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mplotPerplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a8d6e4f05f26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         callbacks=[early_stopping, reduce_lr, csv_logger, checkpointer, plot_history])\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   2137\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2235\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m                     raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if PRINT_TRAIN_PROGRESS_TO_TERMINAL:\n",
    "    reload(sys)\n",
    "\n",
    "small_samples = 200\n",
    "nb_train_samples = small_samples # len(X_train_labels)\n",
    "nb_val_samples = small_samples # len(X_val_labels)\n",
    "print 'Total training samples:', nb_train_samples\n",
    "print 'Total val samples:', nb_val_samples\n",
    "batch_size = 128 # nb_val_samples/2\n",
    "\n",
    "def train(model):\n",
    "    # shuffling of training data True by default\n",
    "    print 'Training...'\n",
    "    history = model.fit_generator(\n",
    "        sampling_generator(nb_train_samples, batch_size), \n",
    "        steps_per_epoch=nb_train_samples/batch_size,\n",
    "        epochs=NB_EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=sampling_generator(nb_val_samples, batch_size, validation=True),\n",
    "        validation_steps=nb_val_samples/batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr, csv_logger, checkpointer, plot_history])\n",
    "\n",
    "    hist = history.history\n",
    "    # print 'Loss:', hist['loss'][0], 'and val loss:', hist['val_loss'][0]\n",
    "    print 'Full training history:\\n', history\n",
    "\n",
    "train(model)\n",
    "plotPerplexity(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('{}.hdf5'.format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #\n",
      "Final str: #!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!\n",
      "Final str: #!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!\n",
      "Final str: #!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!\n",
      "Final str: #!!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!!\n",
      "Final str: #!!!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!!!\n",
      "Final str: #!!!!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!!!!\n",
      "Final str: #!!!!!!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ebeef367642f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mseed_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mgenerate_confession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-ebeef367642f>\u001b[0m in \u001b[0;36mgenerate_confession\u001b[0;34m(model, seed_string)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# get next char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# otherwise wrapped in (1,maxlen+1,len(chars))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"best_tokens:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1748\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/peniel/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-2.7.8-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def convert_sentence_to_ohe(sentence):\n",
    "    x_label = map(lambda x: tokens_indices[x], sentence)\n",
    "    confession = np.zeros(MAX_LEN+1)\n",
    "    confession[:len(x_label)] = x_label\n",
    "    ohe_x = to_categorical(confession, num_classes=NB_CLASSES)\n",
    "    return np.expand_dims(ohe_x, axis=0)\n",
    "\n",
    "def generate_confession(model, seed_string):\n",
    "    # nb chars to preserve\n",
    "    orig_len = len(seed_string) \n",
    "    window_str = seed_string\n",
    "    final_str = seed_string\n",
    "    \n",
    "    for char_nb in range(orig_len, MAX_LEN):\n",
    "        x = convert_sentence_to_ohe(window_str)\n",
    "        \n",
    "        # get next char\n",
    "        preds = model.predict(x)[0] # otherwise wrapped in (1,maxlen+1,len(chars))\n",
    "        best_tokens = np.argmax(preds, axis=1)\n",
    "        print \"best_tokens:\",best_tokens\n",
    "        \n",
    "#         word = \"\"\n",
    "#         for j in best_tokens:\n",
    "#             word += indices_tokens[j]\n",
    "#         print \"predicted Y:\",word\n",
    "        \n",
    "        # char_nb-1 because we want prev val in y matrix (best_tokens). In training we treat y as a shifted \n",
    "            # version of x hence offset -1 here\n",
    "        if char_nb >= WINDOW_SIZE:\n",
    "            next_token = best_tokens[WINDOW_SIZE-1]\n",
    "        else:\n",
    "            next_token = best_tokens[char_nb-1] \n",
    "        \n",
    "        print \"Next_token:\", next_token, \"is:\", indices_tokens[next_token]\n",
    "\n",
    "        # stop symbol\n",
    "        if next_token == STOP_INDEX:\n",
    "            break\n",
    "            \n",
    "        next_char = indices_tokens[next_token]\n",
    "        print 'Current string:', window_str\n",
    "    \n",
    "        if len(window_str) == WINDOW_SIZE:\n",
    "            moveConf = window_str[1:] + next_char\n",
    "            window_str = moveConf\n",
    "            print \"\\n New window_str:\", window_str\n",
    "        else:\n",
    "            window_str += next_char\n",
    "            \n",
    "        final_str += next_char\n",
    "        \n",
    "        print \"Final str:\", final_str\n",
    "          \n",
    "    return final_str\n",
    "\n",
    "seed_string = \"#\"\n",
    "print generate_confession(model, seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
