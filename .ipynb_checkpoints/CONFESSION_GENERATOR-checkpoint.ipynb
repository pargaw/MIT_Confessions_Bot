{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBALS AND INITIAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot import emo_unicode\n",
    "from keras.callbacks import Callback, EarlyStopping, CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, TimeDistributed,Masking \n",
    "from keras.models import load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize #, wordpunct_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import sleep\n",
    "import csv\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "# settings\n",
    "CHAR_BY_CHAR = True # True if doing character-by-character training\n",
    "CUSTOM_NAMES = True # set as True to use CUSTOM_NAME token (and not empty str) for tagged Facebook names\n",
    "MODEL_NAME = 'lstm_512_charchar_likes' # name models descriptively, e.g. with param sizes like lstm_512_hidden\n",
    "PRINT_TRAIN_PROGRESS_TO_TERMINAL = False # set True to see losses change in real-time in terminal\n",
    "WEIGHT_BY_REACTIONS = True\n",
    "LIKES_ONLY = True\n",
    "\n",
    "# parameters\n",
    "# MAX_LEN is defined later\n",
    "HIDDEN_UNITS = 512\n",
    "NB_EPOCHS = 2#20\n",
    "STEP_SIZE = 1\n",
    "WINDOW_SIZE = 10 \n",
    "LSTM_MODEL = True\n",
    " \n",
    "# paths with data based on filtering done in adapted version of Facebook post scraping\n",
    "STATUS_FILEPATH = \"csv_data/beaverconfessions_facebook_statuses.csv\"\n",
    "\n",
    "if CUSTOM_NAMES:\n",
    "    COMMENTS_FILEPATH = \"csv_data/CUSTOM_NAMES/custom_name_token_comments.csv\"\n",
    "else:\n",
    "    COMMENTS_FILEPATH = \"csv_data/FILTERED_NAMES/filtered_names_comments.csv\"\n",
    "\n",
    "# symbols\n",
    "CUSTOM_NAME = 'CUSTOM_NAME'\n",
    "CUSTOM_NUMBER = 'CUSTOM_NUMBER'\n",
    "PADDING_SYMBOL = \"{\"\n",
    "STOP_SYMBOL = \"`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING (TOKENIZATION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaped punctuation: \\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~\n",
      "\n",
      "Test case: ['C', 'U', 'S', 'T', 'O', 'M', '_', 'N', 'A', 'M', 'E', ' ', '6', '.', '8', '6', '7', 't', 'e', 's', 't', '.', ' ', '@', '7', ':', '3', '0', 'p', 'm', ' ', 'i', 'n', ' ', '5', '4', '-', '1', '0', '0', '.', ' ', 'T', 'h', 'e', ' ', 'p', 'r', 'o', 'f', 's', ',', ' ', 't', 'h', 'e', 'y', \"'\", 'r', 'e', ' ', 'c', 'o', 'm', 'i', 'n', 'g', ' ', 'i', 'n', ' ', '3', '.', '.', '2', '.', '.', '1', '.', ' ', 'L', 'E', 'T', 'S', ' ', 'D', 'O', 'O', 'O', ' ', 'T', 'H', 'I', 'S', '!', '!', '!', ' ', '>', ';', ')', '`']\n"
     ]
    }
   ],
   "source": [
    "escaped_punctuation = re.escape(string.punctuation)\n",
    "print 'Escaped punctuation:', escaped_punctuation\n",
    "\n",
    "# insert an OR pipe before each punctuation mark\n",
    "xor_punctuation = '|'.join('{}{}+'.format(escaped_punctuation[x], escaped_punctuation[x+1]) for x in range(0, len(escaped_punctuation), 2))\n",
    "# print 'Delimited punctuation:', xor_punctuation\n",
    "\n",
    "# build regex with variable (order matters!)\n",
    "custom_name = CUSTOM_NAME.lower()\n",
    "course_number = '\\d+\\.\\d{2,3}'\n",
    "multiple_numbers = '\\d+'\n",
    "emoticon_pattern = '|'.join(emoticon for emoticon in emo_unicode.EMOTICONS)\n",
    "space = '\\s'\n",
    "regex_expr = r'(' + '|'.join([custom_name, course_number, multiple_numbers, emoticon_pattern, xor_punctuation, space]) + r')'\n",
    "# print '\\nRegex expression:', regex_expr\n",
    "\n",
    "def replace_digit_with_token(string): \n",
    "    return CUSTOM_NUMBER if string.isdigit() else string\n",
    "\n",
    "def tokenize_str(string, replace_digits=True):\n",
    "    \"\"\"\n",
    "    NLTK tokenizers (word_tokenize, wordpunct_tokenize) are insufficient, \n",
    "    as emoticons and course #s, e.g. 6.111, are important to our dataset.\n",
    "    \n",
    "    This is a custom tokenizer to retain such items in the vocab,\n",
    "        but split up other words containing numbers/punctuation, e.g. 3pm.\n",
    "        \n",
    "    Note that these texts are lowercased by default prior to tokenization.\n",
    "    \"\"\"\n",
    "    if CHAR_BY_CHAR:\n",
    "        tokens = list(string)\n",
    "        \n",
    "    else:\n",
    "        tokens = re.split(regex_expr, string.lower())\n",
    "\n",
    "        # filter out spaces\n",
    "        tokens = [token for token in tokens if token not in [\"\", \" \"]]\n",
    "\n",
    "        # tokens with *just* digits are mapped to CUSTOM_NUMBER by default\n",
    "        if replace_digits:\n",
    "            tokens = map(lambda x: replace_digit_with_token(x), tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# attempt to catch major cases in our dataset\n",
    "sentence = 'CUSTOM_NAME 6.867test. @7:30pm in 54-100. The profs, they\\'re coming in 3..2..1. LETS DOOO THIS!!! >;)' + STOP_SYMBOL\n",
    "print '\\nTest case:', tokenize_str(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max # of tokens in a sentence: 281\n",
      "['#', '9', '5', '4', '1', ' ', 'I', ' ', 'h', 'a', 'd', ' ', 'f', 'e', 'e', 'l', 'i', 'n', 'g', 's', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'l', 'a', 's', 't', ' ', 'y', 'e', 'a', 'r', ',', ' ', 'b', 'u', 't', ' ', 'I', ' ', 'w', 'a', 's', ' ', 'o', 'n', 'l', 'y', ' ', 'a', ' ', 'f', 'r', 'o', 's', 'h', ' ', 't', 'h', 'e', 'n', '.', ' ', 'N', 'o', 'w', ',', ' ', 'y', 'o', 'u', \"'\", 'r', 'e', ' ', 'a', ' ', 's', 'e', 'n', 'i', 'o', 'r', ',', ' ', 'a', 'n', 'd', ' ', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'k', 'n', 'o', 'w', ' ', 'i', 'f', ' ', 'y', 'o', 'u', \"'\", 'd', ' ', 'b', 'e', ' ', 'w', 'i', 'l', 'l', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'm', 'a', 'k', 'e', ' ', 't', 'h', 'e', ' ', 'e', 'm', 'o', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'i', 'n', 'v', 'e', 's', 't', 'm', 'e', 'n', 't', '.', ' ', 'I', ' ', 'k', 'n', 'o', 'w', ' ', 'I', \"'\", 'd', ' ', 'b', 'e', ' ', 'w', 'i', 'l', 'l', 'i', 'n', 'g', ' ', 't', 'o', '.', '`']\n",
      "['#', '9', '5', '4', '0', ' ', 'I', ' ', 'j', 'u', 's', 't', ' ', 'l', 'o', 'o', 'k', 'e', 'd', ' ', 'a', 't', ' ', 'a', 'n', 'o', 't', 'h', 'e', 'r', ' ', 'j', 'u', 'n', 'i', 'o', 'r', \"'\", 's', ' ', 'r', 'e', 's', 'u', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'g', 'o', 't', ' ', 'l', 'e', 'g', 'i', 't', 'i', 'm', 'a', 't', 'e', 'l', 'y', ' ', 'i', 'n', 't', 'i', 'm', 'i', 'd', 'a', 't', 'e', 'd', '.', ' ', 'W', 'e', ' ', 'b', 'o', 't', 'h', ' ', 'd', 'i', 'd', ' ', 't', 'h', 'i', 's', ' ', 'o', 'n', 'e', ' ', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', \"'\", 's', ' ', 't', 'h', 'e', ' ', '\"', 's', 't', 'a', 'r', ' ', 'i', 't', 'e', 'm', '\"', ' ', 'o', 'n', ' ', 'm', 'y', ' ', 'r', 'e', 's', 'u', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'i', 't', \"'\", 's', ' ', 'l', 'i', 't', 'e', 'r', 'a', 'l', 'l', 'y', ' ', 'l', 'e', 'f', 't', ' ', 'o', 'f', 'f', ' ', 'h', 'e', 'r', 's', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 's', 'h', 'e', ' ', 'd', 'i', 'd', 'n', \"'\", 't', ' ', 'h', 'a', 'v', 'e', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ' ', 'r', 'o', 'o', 'm', ' ', 'w', 'i', 't', 'h', ' ', 'a', 'l', 'l', ' ', 'o', 'f', ' ', 'h', 'e', 'r', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'c', 'r', 'a', 'z', 'y', ' ', 's', 't', 'u', 'f', 'f', '.', '`']\n",
      "['#', '9', '5', '3', '7', ' ', 'T', 'o', ' ', 't', 'h', 'e', ' ', 'q', 'u', 'a', 'd', ' ', 'o', 'f', ' ', 'g', 'i', 'r', 'l', 's', ' ', 't', 'h', 'a', 't', ' ', 'l', 'i', 'v', 'e', ' ', 'r', 'i', 'g', 'h', 't', ' ', 'n', 'e', 'x', 't', ' ', 't', 'o', ' ', 'm', 'e', ':', ' ', 'I', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'w', 'i', 's', 'h', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'e', 'w', ' ', 't', 'h', 'a', 't', ' ', 'm', 'y', ' ', 'w', 'a', 'l', 'l', 's', ' ', 'a', 'r', 'e', 'n', \"'\", 't', ' ', 'a', 's', ' ', 's', 'o', 'u', 'n', 'd', 'p', 'r', 'o', 'o', 'f', ' ', 'a', 's', ' ', 'y', 'o', 'u', ' ', 't', 'h', 'i', 'n', 'k', '.', '`']\n",
      "['#', '9', '5', '3', '9', ' ', 'I', ' ', 'h', 'o', 'o', 'k', 'e', 'd', ' ', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'j', 'u', 'n', 'i', 'o', 'r', ' ', 'g', 'u', 'y', ' ', 'i', 'n', ' ', 'a', ' ', 'f', 'r', 'a', 't', 'e', 'r', 'n', 'i', 't', 'y', ' ', 'j', 'u', 's', 't', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 'm', 'y', ' ', 'f', 'r', 'i', 'e', 'n', 'd', ' ', 'd', 'i', 'd', ' ', 't', 'w', 'o', ' ', 'd', 'a', 'y', 's', ' ', 'e', 'a', 'r', 'l', 'i', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'I', ' ', 'd', 'i', 'd', 'n', \"'\", 't', ' ', 'w', 'a', 'n', 't', ' ', 'h', 'e', 'r', ' ', 't', 'o', ' ', 't', 'h', 'i', 'n', 'k', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'b', 'e', 't', 't', 'e', 'r', ' ', 't', 'h', 'a', 'n', ' ', 'm', 'e', '`']\n",
      "['#', '9', '5', '3', '8', ' ', 'W', 'h', 'e', 'n', 'e', 'v', 'e', 'r', ' ', 'm', 'y', ' ', 'f', 'r', 'i', 'e', 'n', 'd', ' ', 'g', 'e', 't', 's', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 'f', 'u', 'c', 'k', 'e', 'd', ' ', 'u', 'p', ' ', 's', 'h', 'e', ' ', 'f', 'o', 'r', 'g', 'e', 't', 's', ' ', 'e', 'n', 'g', 'l', 'i', 's', 'h', ' ', 'a', 'n', 'd', ' ', 's', 't', 'a', 'r', 't', 's', ' ', 'b', 'a', 'b', 'b', 'l', 'i', 'n', 'g', ' ', 'F', 'r', 'e', 'n', 'c', 'h', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'n', ' ', 'i', 'd', 'i', 'o', 't', '`']\n",
      "['#', '9', '5', '3', '6', ' ', 'S', 'o', 'm', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'u', 'p', 'p', 'e', 'r', 'c', 'l', 'a', 's', 's', 'm', 'e', 'n', ' ', 'i', 'n', ' ', 'o', 'u', 'r', ' ', 'f', 'r', 'a', 't', 'e', 'r', 'n', 'i', 't', 'y', ' ', 'r', 'e', 'a', 'l', 'l', 'y', ' ', 's', 't', 'r', 'u', 'g', 'g', 'l', 'e', ' ', 'a', 't', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'g', 'i', 'r', 'l', 's', ' ', 'a', 'p', 'p', 'a', 'r', 'e', 'n', 't', 'l', 'y', ',', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 't', 'h', 'e', 'y', ' ', 's', 't', 'a', 'r', 't', 'e', 'd', ' ', 't', 'h', 'r', 'o', 'w', 'i', 'n', 'g', ' ', 'p', 'a', 'r', 't', 'i', 'e', 's', ' ', 't', 'h', 'a', 't', ' ', 'o', 'n', 'l', 'y', ' ', 'f', 'r', 'e', 's', 'h', 'm', 'a', 'n', ' ', 'g', 'i', 'r', 'l', 's', ' ', '(', 'n', 'o', 't', ' ', 'e', 'v', 'e', 'n', ' ', 'o', 'u', 'r', ' ', 'p', 'l', 'e', 'd', 'g', 'e', 's', ')', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'g', 'e', 't', ' ', 'i', 'n', 't', 'o', '.', '`']\n",
      "['#', '9', '5', '3', '3', ' ', 'I', ' ', 'j', 'u', 's', 't', ' ', 'r', 'e', 'a', 'l', 'i', 'z', 'e', 'd', '-', ' ', 'M', 'I', 'T', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'l', 'e', 't', ' ', 'i', 't', 's', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', 's', ' ', 'c', 'o', 'm', 'p', 'e', 't', 'e', ' ', 'f', 'o', 'r', ' ', 'g', 'r', 'a', 'd', 'e', 's', '.', ' ', 'S', 'o', ' ', 'i', 'n', 's', 't', 'e', 'a', 'd', ',', ' ', 'w', 'e', ' ', 'c', 'o', 'm', 'p', 'e', 't', 'e', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'h', 'o', 'w', ' ', 'm', 'i', 's', 'e', 'r', 'a', 'b', 'l', 'e', ' ', 'w', 'e', ' ', 'c', 'a', 'n', ' ', 'm', 'a', 'k', 'e', ' ', 'o', 'u', 'r', 's', 'e', 'l', 'v', 'e', 's', '.', '`']\n",
      "['#', '9', '5', '3', '5', ' ', 'T', 'h', 'a', 'n', 'k', ' ', 'y', 'o', 'u', ' ', 'S', 'e', 'n', 'i', 'o', 'r', ' ', 'H', 'a', 'u', 's', ' ', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 's', '!', ' ', 'T', 'h', 'a', 'n', 'k', 's', ' ', 't', 'o', ' ', 'y', 'o', 'u', ' ', 'g', 'u', 'y', 's', ',', ' ', 'I', \"'\", 'm', ' ', 'l', 'i', 'v', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'a', 'n', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', ' ', 'g', 'r', 'a', 'd', ' ', 'd', 'o', 'r', 'm', '!', '`']\n",
      "['#', '9', '5', '3', '4', ' ', 'I', \"'\", 'm', ' ', 'm', 'o', 'r', 'e', ' ', 'p', 'r', 'o', 'u', 'd', ' ', 'o', 'f', ' ', 'h', 'o', 'w', ' ', 'm', 'y', ' ', 'b', 'i', 't', 'c', 'o', 'i', 'n', ' ', 'v', 'a', 'l', 'u', 'e', ' ', 'h', 'a', 's', ' ', 'g', 'r', 'o', 'w', 'n', ' ', 't', 'h', 'a', 'n', ' ', 'h', 'o', 'w', ' ', 'I', \"'\", 'v', 'e', ' ', 'g', 'r', 'o', 'w', 'n', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'l', 'a', 's', 't', ' ', '3', ' ', 'y', 'e', 'a', 'r', 's', '.', '`']\n",
      "['#', '9', '5', '2', '8', ' ', 'I', 't', ' ', 'g', 'e', 't', 's', ' ', 's', 'o', ' ', 'h', 'o', 't', ' ', 'i', 'n', ' ', 'm', 'y', ' ', 'd', 'o', 'r', 'm', '.', ' ', 'S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ',', ' ', 'I', ' ', 'j', 'u', 's', 't', ' ', 'w', 'a', 'n', 't', ' ', 't', 'a', 'k', 'e', ' ', 'o', 'f', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'y', ' ', 'c', 'l', 'o', 't', 'h', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'h', 'a', 'v', 'e', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'b', 'o', 'y', 's', ' ', 'i', 'n', ' ', 'm', 'y', ' ', 'l', 'i', 'c', 'k', ' ', 'm', 'y', ' ', 'n', 'i', 'p', 'p', 'l', 'e', 's', ' ', 't', 'o', ' ', 'c', 'o', 'o', 'l', ' ', 'm', 'e', ' ', 'd', 'o', 'w', 'n', '.', ' ', 'T', 'h', 'e', 's', 'e', ' ', 'f', 'r', 'e', 's', 'h', 'm', 'a', 'n', ' ', 'b', 'o', 'y', 's', ' ', 'a', 'r', 'e', ' ', 's', 'o', ' ', 'c', 'u', 't', 'e', '.', ' ', 'A', 's', ' ', 'a', ' ', 's', 'e', 'n', 'i', 'o', 'r', ',', ' ', 'I', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 't', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'm', 'y', ' ', 'r', 'e', 's', 'p', 'o', 'n', 's', 'i', 'b', 'i', 'l', 'i', 't', 'y', ' ', 't', 'o', ' ', 't', 'e', 'a', 'c', 'h', ' ', 't', 'h', 'e', 'm', ' ', 'h', 'o', 'w', ' ', 't', 'o', ' ', 'p', 'l', 'e', 'a', 's', 'u', 'r', 'e', ' ', 't', 'h', 'e', ' ', 'f', 'e', 'm', 'a', 'l', 'e', ' ', 'b', 'o', 'd', 'y', '.', '`']\n",
      "\n",
      "Max # of reactions: 915.0\n",
      "\n",
      "number of classes: 94\n",
      "\n",
      "The padding index is: 90\n",
      "\n",
      "The stop index is: 63\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_and_vocabulary(filepath, vocabulary, datatype, max_nb_tokens, reactions):\n",
    "    \"\"\"\n",
    "    Read individual sentences into memory, and generate vocabulary.\n",
    "    \n",
    "    If CHAR_BY_CHAR, the vocabulary will hold single characters, \n",
    "        e.g. a-zA-Z and punctuation.\n",
    "    Else, it will contain whole words and Unicode 'phrases',\n",
    "        e.g. :\\'(, as split by our custom tokenizer.\n",
    "    \"\"\"\n",
    "    dt = \"{}_message\".format(datatype)\n",
    "    \n",
    "    if LIKES_ONLY:\n",
    "        reaction_header = 'num_likes'\n",
    "    else:\n",
    "        reaction_header = 'num_reactions'\n",
    "    \n",
    "    with open(filepath, \"rU\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        tokens = []\n",
    "        \n",
    "        for status in reader:\n",
    "            # we distinguish between comments and statuses with header cols in the CSVs\n",
    "#             if dt not in status:\n",
    "#                 print 'test'\n",
    "#                 # 2 rows are read at once from STATUS_FILEPATH for some reason...\n",
    "#                 # handled with monkey patching \n",
    "#                 msg1, msg2 = status.items()[1]\n",
    "#                 # num_reacts1, num_reacts2 = status[reaction_header] # ...?\n",
    "                \n",
    "#                 # append stop symbol so model has explicit marker for ending\n",
    "#                 msg1 += STOP_SYMBOL\n",
    "#                 msg2 += STOP_SYMBOL\n",
    "                \n",
    "#                 # we define elts as the unit we are training on, e.g. char vs. word \n",
    "#                 elts_in_sentence1 = tokenize_str(msg1)\n",
    "#                 elts_in_sentence2 = tokenize_str(msg2)\n",
    "                \n",
    "#                 max_nb_tokens = max(max_nb_tokens, max(len(elts_in_sentence1), len(elts_in_sentence2)))\n",
    "                 \n",
    "#                 for elt in elts_in_sentence1:\n",
    "#                     vocabulary.add(elt)\n",
    "#                 for elt in elts_in_sentence2:\n",
    "#                     vocabulary.add(elt) \n",
    "                    \n",
    "#                 # build list of tokens in local memory\n",
    "#                 tokens.append(elts_in_sentence1)\n",
    "#                 tokens.append(elts_in_sentence2)\n",
    "#             else:\n",
    "            msg = status[dt]\n",
    "            num_reacts = float(status[reaction_header])\n",
    "\n",
    "            msg += STOP_SYMBOL \n",
    "            elts_in_sentence = tokenize_str(msg)\n",
    "            max_nb_tokens = max(max_nb_tokens, len(elts_in_sentence))\n",
    "\n",
    "            for elt in elts_in_sentence:\n",
    "                vocabulary.add(elt)\n",
    "\n",
    "            tokens.append(elts_in_sentence)\n",
    "            reactions.append(num_reacts)\n",
    "                    \n",
    "    return vocabulary, tokens, max_nb_tokens, reactions\n",
    "    \n",
    "# load comments and statuses separately at first\n",
    "max_nb_tokens = 0\n",
    "vocabulary, comment_tokens, max_nb_tokens, comment_reacts = load_dataset_and_vocabulary(COMMENTS_FILEPATH, set([]), \"comment\", max_nb_tokens, [])\n",
    "vocabulary, status_tokens, max_nb_tokens, reacts = load_dataset_and_vocabulary(STATUS_FILEPATH, vocabulary, \"status\", max_nb_tokens, comment_reacts)\n",
    "MAX_LEN = max_nb_tokens\n",
    "\n",
    "print 'Max # of tokens in a sentence:', MAX_LEN\n",
    "# print '\\nSample of 10 processed sentences:'\n",
    "for status in status_tokens[:10]:\n",
    "    print status\n",
    "    \n",
    "max_num_reacts = max(reacts)\n",
    "print '\\nMax # of reactions:', max_num_reacts\n",
    "\n",
    "# create vocabulary of characters found in data\n",
    "if not CHAR_BY_CHAR:\n",
    "    vocabulary.add(CUSTOM_NAME)\n",
    "    vocabulary.add(CUSTOM_NUMBER)\n",
    "    \n",
    "# these symbols did not exist in the original training set,\n",
    "# so we can safely add them \n",
    "# NOTE: changed from insert(0,_), since we were adding duplicates\n",
    "vocabulary.add(STOP_SYMBOL)\n",
    "vocabulary.add(PADDING_SYMBOL)\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "\n",
    "NB_CLASSES = len(vocabulary)\n",
    "print '\\nnumber of classes:', NB_CLASSES\n",
    "\n",
    "tokens_indices = dict((t, i) for i, t in enumerate(vocabulary))\n",
    "indices_tokens = {v: k for k, v in tokens_indices.iteritems()}\n",
    "PADDING_INDEX = tokens_indices[PADDING_SYMBOL]\n",
    "STOP_INDEX = tokens_indices[STOP_SYMBOL]\n",
    "print '\\nThe padding index is:', PADDING_INDEX\n",
    "print '\\nThe stop index is:', STOP_INDEX\n",
    "\n",
    "input_type = \"character by character\" if CHAR_BY_CHAR else \"word by word\"\n",
    "# print \"For\", input_type, \", we have this vocabulary:\", vocabulary, \"\\n of size\", NB_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD TRAINING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized sentence:\n",
      "['#', '3', '0', '5', '6', '4', ' ', 'S', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 'i', 't', \"'\", 's', ' ', 'w', 'h', 'a', 't', ' ', 'Y', 'O', 'U', ' ', 'm', 'a', 'k', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', ',', ' ', 'm', 'a', 'y', 'b', 'e', ' ', 'y', 'o', 'u', ' ', 'j', 'u', 's', 't', ' ', 'g', 'r', 'e', 'w', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'a', 'r', 'n', 'e', 'd', ' ', 'm', 'o', 'r', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 'i', 't', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', 'm', '`']\n",
      "\n",
      "number of samples: 387097\n",
      "Sample X: ['#', '3', '0', '5', '6', '4', ' ', 'S', 'o', 'm']\n",
      "Sample Y: ['3', '0', '5', '6', '4', ' ', 'S', 'o', 'm', 'e']\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-bb95b67b9b27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mX_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_tokens_to_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;31m#X_labels, y_labels = X, Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Sample X labels:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-bb95b67b9b27>\u001b[0m in \u001b[0;36mconvert_tokens_to_int\u001b[1;34m(X, Y, nb_samples)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# extra 1 represents space for the stop symbol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mX_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0my_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# merge comments and statuses\n",
    "tokens = []\n",
    "tokens.extend(comment_tokens)\n",
    "tokens.extend(status_tokens)\n",
    "tokens = np.array(tokens)\n",
    "\n",
    "# shuffle dataset (commented out b/c we shuffle during training anyway)\n",
    "# random_permutation = np.random.permutation(len(tokens))\n",
    "# tokens = tokens_arr[random_permutation]\n",
    "\n",
    "print 'Sample tokenized sentence:'\n",
    "# print tokens_arr\n",
    "print tokens[0]\n",
    "print\n",
    "\n",
    "def generate_X_and_Y(sentences):\n",
    "    \"\"\"\n",
    "    X = sub_sentences, Y = next_sub_sentences\n",
    "    Y is simply X shifted over by step_size.\n",
    "    We want sub sentences per sentence to create multiple samples.\n",
    "    \"\"\"\n",
    "    sub_sentences = []\n",
    "    next_sub_sentences = []\n",
    "    sub_reactions = []\n",
    "    for sentence_nb, sentence in enumerate(sentences):\n",
    "        sentence_reacts = reacts[sentence_nb]\n",
    "        for i in range(0, len(sentence) - WINDOW_SIZE, STEP_SIZE):\n",
    "            sub_sentences.append(sentence[i : i+WINDOW_SIZE])\n",
    "            next_sub_sentences.append(sentence[(i+STEP_SIZE) : (i+STEP_SIZE)+WINDOW_SIZE])\n",
    "            sub_reactions.append(sentence_reacts)\n",
    "    \n",
    "    return sub_sentences, next_sub_sentences, sub_reactions\n",
    "\n",
    "X, Y, X_reacts = generate_X_and_Y(tokens)\n",
    "nb_samples = len(X)\n",
    "print 'number of samples:', nb_samples\n",
    "print 'Sample X:', X[0]\n",
    "print 'Sample Y:', Y[0]\n",
    "print \n",
    "\n",
    "def convert_tokens_to_int(X, Y, nb_samples):\n",
    "    \"\"\"\n",
    "    Convert token to integer representations.\n",
    "    \n",
    "    Populate zero-filled label matrices with ints,\n",
    "        so end result is padded and ready for training.\n",
    "    \"\"\"\n",
    "    # any values not filled in later represent padding \n",
    "    # extra 1 represents space for the stop symbol\n",
    "    input_shape = (nb_samples, MAX_LEN+1)\n",
    "    X_labels = np.zeros(input_shape)\n",
    "    y_labels = np.zeros(input_shape)\n",
    "\n",
    "    for sample_nb in range(nb_samples):\n",
    "        x_label = map(lambda x: tokens_indices[x], X[sample_nb]) \n",
    "        y_label = map(lambda x: tokens_indices[x], Y[sample_nb])\n",
    "\n",
    "        X_labels[sample_nb][:len(x_label)] = x_label\n",
    "        y_labels[sample_nb][:len(y_label)] = y_label\n",
    "    \n",
    "    return X_labels, y_labels\n",
    "\n",
    "X_labels, y_labels = convert_tokens_to_int(X, Y, nb_samples)\n",
    "#X_labels, y_labels = X, Y\n",
    "print 'Sample X labels:\\n', X_labels[:1]\n",
    "print 'Sample Y labels:\\n', y_labels[:1]\n",
    "print '\\nTotal # of training samples:', nb_samples\n",
    "\n",
    "# split data into train and val sets\n",
    "X_train_labels, X_val_labels, y_train_labels, y_val_labels = train_test_split(X_labels, y_labels, test_size=0.3)\n",
    "num_train = len(X_train_labels)\n",
    "X_train_reacts = X_reacts[:num_train]\n",
    "X_val_reacts = X_reacts[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PRINT_TRAIN_PROGRESS_TO_TERMINAL:\n",
    "    reload(sys)\n",
    "\n",
    "def validate_data(X, y):\n",
    "    # assert at least one nonzero value in each OHE sample\n",
    "    assert(False not in np.any(X>0, axis=1))\n",
    "    assert(False not in np.any(y>0, axis=1))\n",
    "\n",
    "def sampling_generator(epoch_size, batch_size, validation=False):  \n",
    "    \"\"\"\n",
    "    Takes in labels of int values, e.g. [1. 2. 5. 9. 1. 0. 0. ...]\n",
    "    These labels are already padded with 0s to a predetermined maxlen.\n",
    "    We convert each sample X and y to one hot versions, and create batches.\n",
    "    Batch generation is necessary to avoid MemoryError.\n",
    "    \"\"\"\n",
    "    if validation:\n",
    "        X_labels = X_val_labels \n",
    "        y_labels = y_val_labels\n",
    "        X_reacts = X_val_reacts\n",
    "    else:\n",
    "        X_labels = X_train_labels\n",
    "        y_labels = y_train_labels\n",
    "        X_reacts = X_train_reacts\n",
    "    \n",
    "    while True:\n",
    "        # hand off data in batches\n",
    "        for i in range(int(epoch_size/batch_size)):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, epoch_size)\n",
    "            true_batch_size = end - start\n",
    "\n",
    "            # fresh batch\n",
    "            batch_X = []    # all one-hot inputs for batch\n",
    "            batch_Y = []    # all one-hot labels for batch\n",
    "            batch_shape = (true_batch_size, MAX_LEN+1, NB_CLASSES)\n",
    "            \n",
    "            X_label_sample = X_labels[start:end]   # batch_size number of samples\n",
    "            X_label_sample_reacts = X_reacts[start:end]   \n",
    "\n",
    "            # one hot encode\n",
    "            batch_X = to_categorical(X_label_sample, num_classes=NB_CLASSES).reshape(batch_shape)\n",
    "            batch_Y = to_categorical(y_labels[start:end], num_classes=NB_CLASSES).reshape(batch_shape)\n",
    "            \n",
    "            total_nb_one_hots = X_label_sample.size  # this is just batch_size * number of timesteps * nb_classes\n",
    "    \n",
    "            sample_weights = np.ones((total_nb_one_hots, 1))\n",
    "            \n",
    "            temp_reshaped_Xs = np.ravel(X_label_sample).reshape(-1)\n",
    "            padded_positions = np.where(temp_reshaped_Xs == PADDING_INDEX)\n",
    "            sample_weights[padded_positions] = 0 # 28100 x 1\n",
    "            sample_weights = sample_weights.reshape(true_batch_size, MAX_LEN+1)\n",
    "            print sample_weights\n",
    "            \n",
    "            if WEIGHT_BY_REACTIONS:\n",
    "                react_weights = np.array(X_label_sample_reacts).reshape(-1,1) / max_num_reacts + 1.0\n",
    "                sample_weights = np.multiply(sample_weights, react_weights)\n",
    "            \n",
    "            yield (batch_X, batch_Y, sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "Model is made!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 282, 512)          1243136   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 282, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 282, 94)           48222     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 282, 94)           0         \n",
      "=================================================================\n",
      "Total params: 3,390,558\n",
      "Trainable params: 3,390,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    if LSTM_MODEL:\n",
    "        print('Building LSTM model...')\n",
    "        model = Sequential()\n",
    "        # model.add(Masking(input_shape=(MAX_LEN+1,1)))\n",
    "        model.add(LSTM(512, return_sequences=True, input_shape=(MAX_LEN+1,NB_CLASSES)))\n",
    "        model.add(LSTM(512, return_sequences=True))\n",
    "        # model.add(Dropout(0.2))\n",
    "        model.add(TimeDistributed(Dense(NB_CLASSES)))\n",
    "        model.add(Activation('softmax'))\n",
    "        # if we want to apply sample weights to metrics, we need to specify weighted_metrics=[list of metrics]\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.0), sample_weight_mode=\"temporal\")   \n",
    "    else:\n",
    "        print('Building GRU model...')\n",
    "        model = Sequential()\n",
    "        model.add(GRU(512, return_sequences=True, input_shape=(MAX_LEN+1, NB_CLASSES)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(GRU(512, return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NB_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.0), sample_weight_mode=\"temporal\")\n",
    "\n",
    "    print ('Model is made!')\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALLBACKS (bells + whistles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlotHistory(Callback):\n",
    "    def __init__(self, run_name):\n",
    "        self.run_name = run_name \n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # create loss and perplexity plot\n",
    "        loss_handles = []\n",
    "        train_perp = []\n",
    "        val_perp = []\n",
    "        for key in self.history:           \n",
    "            if key != \"lr\":\n",
    "                l, = plt.plot(self.history[key], label=key)\n",
    "                loss_handles.append(l)\n",
    "\n",
    "        plt.title('Losses and metrics for {}'.format(self.run_name))    \n",
    "        plt.ylabel('loss')\n",
    "        plt.yscale('symlog')\n",
    "        plt.legend([\"Training Loss\",\"Validation Loss\"], fontsize=8, loc='upper right')          \n",
    "        plt.savefig('{}_plot.jpg'.format(self.run_name))        \n",
    "        plt.clf()\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='{}.hdf5'.format(MODEL_NAME), verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('{}.log'.format(MODEL_NAME))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001, verbose=1)\n",
    "plot_history = PlotHistory(MODEL_NAME)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, epsilon=1e-4, min_lr=1E-6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotPerplexity(model):\n",
    "    f = open(model+'.log','r')\n",
    "    epoch_X = []\n",
    "    train_perplex = []\n",
    "    val_perplex = []\n",
    "\n",
    "    for line in f.readlines()[1:]:\n",
    "        data = line.strip().split(\",\")\n",
    "        epoch_X.append(int(data[0]))\n",
    "        train_perplex.append(2**(float(data[1])))\n",
    "        val_perplex.append(2**(float(data[3])))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "\n",
    "    ax.plot(epoch_X, train_perplex)\n",
    "    ax.plot(epoch_X, val_perplex)\n",
    "    plt.legend(['Training perplexity', 'Validation perplexity'], fontsize=8, loc='upper right')\n",
    "    plt.title('Perplexities for {}'.format(model))\n",
    "    plt.savefig('{}_perplexity.jpg'.format(model))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN / LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8d6e4f05f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Full training history:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mplotPerplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a8d6e4f05f26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         callbacks=[early_stopping, reduce_lr, csv_logger, checkpointer, plot_history])\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   2137\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2235\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m                     raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if PRINT_TRAIN_PROGRESS_TO_TERMINAL:\n",
    "    reload(sys)\n",
    "\n",
    "small_samples = 200\n",
    "nb_train_samples = small_samples # len(X_train_labels)\n",
    "nb_val_samples = small_samples # len(X_val_labels)\n",
    "print 'Total training samples:', nb_train_samples\n",
    "print 'Total val samples:', nb_val_samples\n",
    "batch_size = 128 # nb_val_samples/2\n",
    "\n",
    "def train(model):\n",
    "    # shuffling of training data True by default\n",
    "    print 'Training...'\n",
    "    history = model.fit_generator(\n",
    "        sampling_generator(nb_train_samples, batch_size), \n",
    "        steps_per_epoch=nb_train_samples/batch_size,\n",
    "        epochs=NB_EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=sampling_generator(nb_val_samples, batch_size, validation=True),\n",
    "        validation_steps=nb_val_samples/batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr, csv_logger, checkpointer, plot_history])\n",
    "\n",
    "    hist = history.history\n",
    "    # print 'Loss:', hist['loss'][0], 'and val loss:', hist['val_loss'][0]\n",
    "    print 'Full training history:\\n', history\n",
    "\n",
    "train(model)\n",
    "plotPerplexity(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('{}.hdf5'.format(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #\n",
      "Final str: #!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!\n",
      "Final str: #!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!\n",
      "Final str: #!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!\n",
      "Final str: #!!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!!\n",
      "Final str: #!!!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!!!\n",
      "Final str: #!!!!!!\n",
      "best_tokens: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "Next_token: 0 is: !\n",
      "Current string: #!!!!!!\n",
      "Final str: #!!!!!!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ebeef367642f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mseed_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mgenerate_confession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-ebeef367642f>\u001b[0m in \u001b[0;36mgenerate_confession\u001b[0;34m(model, seed_string)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# get next char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# otherwise wrapped in (1,maxlen+1,len(chars))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mbest_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"best_tokens:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1748\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/peniel/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-2.7.8-64/scan_perform/mod.cpp:4490)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/peniel/anaconda/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def convert_sentence_to_ohe(sentence):\n",
    "    x_label = map(lambda x: tokens_indices[x], sentence)\n",
    "    confession = np.zeros(MAX_LEN+1)\n",
    "    confession[:len(x_label)] = x_label\n",
    "    ohe_x = to_categorical(confession, num_classes=NB_CLASSES)\n",
    "    return np.expand_dims(ohe_x, axis=0)\n",
    "\n",
    "def generate_confession(model, seed_string):\n",
    "    # nb chars to preserve\n",
    "    orig_len = len(seed_string) \n",
    "    window_str = seed_string\n",
    "    final_str = seed_string\n",
    "    \n",
    "    for char_nb in range(orig_len, MAX_LEN):\n",
    "        x = convert_sentence_to_ohe(window_str)\n",
    "        \n",
    "        # get next char\n",
    "        preds = model.predict(x)[0] # otherwise wrapped in (1,maxlen+1,len(chars))\n",
    "        best_tokens = np.argmax(preds, axis=1)\n",
    "        print \"best_tokens:\",best_tokens\n",
    "        \n",
    "#         word = \"\"\n",
    "#         for j in best_tokens:\n",
    "#             word += indices_tokens[j]\n",
    "#         print \"predicted Y:\",word\n",
    "        \n",
    "        # char_nb-1 because we want prev val in y matrix (best_tokens). In training we treat y as a shifted \n",
    "            # version of x hence offset -1 here\n",
    "        if char_nb >= WINDOW_SIZE:\n",
    "            next_token = best_tokens[WINDOW_SIZE-1]\n",
    "        else:\n",
    "            next_token = best_tokens[char_nb-1] \n",
    "        \n",
    "        print \"Next_token:\", next_token, \"is:\", indices_tokens[next_token]\n",
    "\n",
    "        # stop symbol\n",
    "        if next_token == STOP_INDEX:\n",
    "            break\n",
    "            \n",
    "        next_char = indices_tokens[next_token]\n",
    "        print 'Current string:', window_str\n",
    "    \n",
    "        if len(window_str) == WINDOW_SIZE:\n",
    "            moveConf = window_str[1:] + next_char\n",
    "            window_str = moveConf\n",
    "            print \"\\n New window_str:\", window_str\n",
    "        else:\n",
    "            window_str += next_char\n",
    "            \n",
    "        final_str += next_char\n",
    "        \n",
    "        print \"Final str:\", final_str\n",
    "          \n",
    "    return final_str\n",
    "\n",
    "seed_string = \"#\"\n",
    "print generate_confession(model, seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
